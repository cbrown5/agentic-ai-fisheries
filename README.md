


# Automating fisheries modelling with agentic AI

Christopher J. Brown, Richard Takyi, Alexandre Tisseaux, Leigha Aitken

Code to support the study. 

## Summary

Fisheries modelling can take years of specialist training to learn, and the availability of suitably skilled modellers is one of the bottlenecks for delivering science that can inform management. Agentic AI systems automate complex computing programming workflows and they will increasingly be used to accelerate the delivery of fisheries modelling for management. However, questions remain about the quality of results from AI agents, because they can create syntactically accurate but logically flawed code, and they do not response consistently if the same question is asked repeatedly. Here, we tested whether a general-purpose agentic AI (Roo Code) can write computer code and interpret results to complete three common ecological and fisheries modelling workflows: (1) fitting a von-Bertalanffy growth curve to data, (2) writing code interpreting results of a generalized linear model relating fish abundance to habitat, and (3) completing a yield per recruit analysis. We used replicate prompts and a structured evaluation rubric to ask if the AI results were accurate and if responses were consistent. The agent successfully and accurately completed all three tasks in some replicates; however, there was variability in the outcomes across replicated agents. Of the two large language models we tested, Claude Sonnet 4.0 performed accurately and consistently across all three tasks. Our findings show that general purpose agentic AI systems can accurately complete ecological and fisheries modelling, but that further work is needed to ensure consistency of results across different replicates. We further show how to design prompts and workflows to achieve more consistent and accurate results and discuss the importance of ensuring human oversight for credibility of modelling that is used in management.

# Directory Overview

Below is an overview of the main sections and folders in this repository. Each section includes a brief explanation of its contents and purpose.

## Top-level Files

- **README.md**  
	This file. Project overview and documentation.

- **manuscript.qmd**  
    Main manuscript file for the project write-up, including analysis.

- **Title-page.qmd**  
	Title page for the manuscript.

- **references.bib**  
	Bibliography and references for the manuscript.

## Private/
Contains any private or sensitive files not intended for sharing.

## Scripts/
Main folder for all model scripts, test cases, and outputs. Subfolders include:

### Scripts/glm-test-case/
Test case for generalized linear models (GLM). Includes:
- `glm-readme.md`, `initial-prompt.md`: Documentation and prompts for the test case.
- `data/`: Input data for the GLM test case.
- `outputs/plots/`: Output plots generated by the test case.

### Scripts/glm-validation-case/
Expert validated code and results for GLM models. Includes:
- `glm-validation.R`: R script for validation.
- `rubric.csv`, `rubric.md`: Rubric for evaluating results (was further updated later on). 
- `summary_plots/`, `verification_plots/`: Plots for model validation and verification.

### Scripts/vbf-test-case/
Test case for von Bertalanffy (VBF) parameter estimation. Includes example data, prompts, and outputs.

### Scripts/vbf-test-case-simple/
Simplified version of the von Bertalanffy test case with streamlined data and prompts.

### Scripts/vbf-validation-case/
Validation scripts and outputs for VBF models. Includes:
- `fit-vbf-FSA.R`, `fit-vbf.R`: R scripts for fitting VBF models using different approaches.
- `parameter-validation.csv`: Validation results for VBF parameter estimates.

### Scripts/ypr-test-case/
Test case for Yield Per Recruit (YPR) analysis. Includes:
- `initial-prompt.md`, `ypr-readme.md`: Documentation and prompts for the test case.
- `ypr_reference_points.csv`: Reference point data for YPR analysis.
- `data/`, `outputs/plots/`, `scripts/`: Input data, output plots, and analysis scripts.

### Scripts/ypr-validation-case/
Expert validated code and results for YPR models. Includes:
- `Version2-01_ypr_parameters_functions.R`, `Version2-02_ypr_analysis.R`: R scripts for YPR analysis.
- `ypr-functions.R`, `ypr-validation.R`: Additional validation and function scripts.
- `ypr_reference_points.csv`: Reference point validation data.


## Shared/
Shared resources and data for the project. Contains:

### Shared/Outputs/
Analysis outputs and figures. Includes:
- `fig2-vbf_Claude-Kimi-simple.png`, `fig2-vbf.png`: von Bertalanffy growth curve comparison figures.
- `figure-3.png`, `figure-4_ypr.png`, `figure-4-YPR-quant_and_qual.png`: Analysis result figures.
- `figure-5-cost-accuracy.png`: Cost-accuracy trade-off analysis figure.
- `glm_summary_heatmap.png`: GLM results summary heatmap.
- `yield_per_recruit-validation.png`: YPR validation results figure.
- `OLD/`: Archived older versions of output files.

### Figure Files (Top-level in Shared/)
- `Figure-comparison-png.png`: Comparison figure in PNG format.
- `Figure1- CB.pdf`, `Figure1- CB.pptx`, `Figure1.png`: Main manuscript Figure 1 in various formats.


## Workflow for running the agent

- Copy files in the test case to a separate project directory (we need to keep them quarantined from the main project to prevent context being shared)
- Open VScode and Roo Code
- Check Roo code autoapprove is on with right approvals as described in the study
- Pick model and set to 'code' mode. 
- Paste the 'initial-prompt' into the chat for Roo Code. 
- Run the test case
- Save the chat log
- Record tokens in, tokens out and cost.
- Mark model results against rubric (if using) or run analysis scripts to evaluate results. 

Note that the agentic AI runs are kept in a separate project directory to the main project. Results tables are then copied over to this project directory for analysis. 
