


# Automating fisheries modelling with agentic AI

Christopher J. Brown, Richard Takyi, Alexandre Tisseaux, Leigha Aitken....

## TODO
- Finish writing methods
- re-run YPR with 15 removed. 
- Edit methods and add references 
- Tokens vs accuracy summary graph. Graph showing one facet for each case-study. Each facet has an x-y with points. x is cost, y is the overall accuracy, colours are the two models. So would need to pull all data in together, then just average accuracy scores across all the criteria, so we get one accuracy score per run - ????
- Write results - Chris with help?
- Write discussion (to do later)
- Add a barramundi SP model with enironmental variation? 

- Leigha to make a compilation of example graphs showing the 'ideal' ones I made versus samples of good and bad results from the AI. Don't make the graph too busy, we just want to illustrate the variety of outcomes. 

YPR data and plotting code currently on the onedrive. 

## Summary

Fisheries modelling can take years of specialist training to learn. Agentic AI systems automate complex computing programming workflows and could lower the technical barrier for fisheries modelling. However, questions remain about the quality of AI derived models. Here we test whether agentic AI can write computer code to complete three common types of fisheries models. We test an agentic AI system (Roo Code) on its ability to complete three fisheries modelling workflows, from data analysis to report write-up: (1) VB parameter estimation, (2) generalized linear modelling of fish-habitat relationships, (3) spawner per recruit analysis. We use replicate prompts and a rubric to evaluate the AI generated reports. We find that... We show how careful prompting of the AI system can deliver high quality reports for fisheries modelling problems. Our results show that agentic AI systems can already complete complex fisheries workflows, particularly if users provide context-rich prompts. We discuss implications for fisheries science including equity of access to technical expertise and the immediate priority of increasing transparency of AI use in fisheries science, potential pitfalls, and the need for a community of practice around prompting.  

## Plan 

Look at three models:
VB parameter estimation - Testing. 
I have made a 'simple' and 'full' version. For now using the 'simple' version. Could do another one where I attach FSA instructions to the prompt.
Then re-run without code 
GLM of fish habitat. Done, mark Kimi K2 results

VPA/or SPR WRite plan and run 

Full YPR: https://haddonm.github.io/URMQMF/simple-population-models.html#simple-yield-per-recruit


## Workflow for the agent

- Copy files in the test case to a separate agentic AI project directory (we need to keep them quarantined from the main project)
- Check Roo code autoapprove is on with right approvals
- Pick model and set to 'code' mode. 
- Run the test case
- Save the chat log
- Record tokens in, tokens out and cost.
- Mark model results against rubric (if using) or run analysis scripts to evaluate results. 

Note that the agentic AI runs are kept in a separate project directory to the main project. Results tables are then copied over to this project directory for analysis. 

# Directory Overview

Below is an overview of the main sections and folders in this repository. Each section includes a brief explanation of its contents and purpose.

## Top-level Files


- **README.md**  
	This file. Project overview and documentation.

- **manuscript.qmd**  
    Main manuscript file for the project write-up, including analysis

- **hints-tips-spec-writing.qmd, Manuscript-outline.qmd,  Title-page.qmd**  
	Drafts, outlines, and main manuscript files for the project write-up.

- **references.bib**  
	Bibliography and references for the manuscript.
- **test-case-notes.qmd**  
	Notes and observations from running test cases.

## Private/
Contains any private or sensitive files not intended for sharing.

## Scripts/
Main folder for all model scripts, test cases, and outputs. Subfolders include:

### Scripts/glm-test-case/
Test case for generalized linear models (GLM). Includes:
- `glm-readme.md`, `initial-prompt.md`: Documentation and prompts for the test case.
- `data/`: Input data for the GLM test case.
- `outputs/plots/`: Output plots generated by the test case.

### Scripts/glm-validation-case/
Expert validated code and results for GLM models. Includes:
- `glm-validation.R`: R script for validation.
- `rubric.csv`, `rubric.md`: Rubric for evaluating results (was further updated later on). 
- `summary_plots/`, `verification_plots/`: Plots for model validation and verification.

### Scripts/vbf-test-case/
Test case for von Bertalanffy (VBF) parameter estimation. Includes example data, prompts, and outputs.

### Scripts/vbf-validation-case/
Validation scripts and outputs for VBF models.

### Scripts/results/
Analyse results of Agent runs, e.g., `analyse-glm.R`.

## Shared/
Shared resources and data for the project. Subfolders include:

### Shared/Data/
Datasets and CSV files used across test cases. May include old versions in `OLD/`.

#### glm-test-case/

`glm-class-levels.csv`: Questions for the rubric along with levels of answers. This is used to evaluate the AI's performance on the GLM test case.

`glm-question-types.csv`: Questions for the rubric along with types of answers split into three categories for summarizing. 

`glm-test-case-results.csv`: Results for all agent runs, marked against the rubric. 

### Shared/Manuscripts/
Shared manuscript drafts and related files. May include archived versions in `OLD/`.



### Shared/Materials/
Shared materials and outputs, with possible archived content in `OLD/`.

### Shared/Outputs/
General outputs from analyses, with older versions in `OLD/`.

---
You can expand each section above to provide more detail or add explanations for new folders as the project evolves.


