---
title: "Automating fisheries modelling with agentic AI"
bibliography: references.bib
# csl: oneearth.csl
editor: source
format: docx
    #reference-doc: template.docx
---

Journal: Fish and Fisheries

## Abstract

-   250 words

-   6 keywords - alphabetical order

Fisheries modelling can take years of specialist training to learn and the availability of suitably skilled modellers is one of the bottlenecks for delivering science that can inform management. Agentic AI systems automate complex computing programming workflows and could accelerate the creation of timely management advice from fisheries models. However, questions remain about the quality of AI derived models. Here we test whether AI agents can write computer code and interpret results satisifactorily to complete three common types of ecological and fisheries models. We test a general purpose agentic AI system (Roo Code) on its ability to complete three fisheries modelling workflows, from data analysis to report write-up: (1) von-Bertalanffy parameter estimation, (2) generalized linear modelling of fish-habitat relationships, (3) yield per recruit analysis. We use replicate prompts and a rubric to evaluate the AI generated reports. We find the agent was able to fully and accurately complete all three tasks, however there was variability in outcomes across replicated agents. Of the two large language models we tested, Claude Sonnet 4 performed highly and consistently across all three tasks. Our results show that general purpose AI agents can deliver high quality reports for fisheries and ecological modelling questions. We further show how to design prompts and workflows to achieve consistent and accurate results. We discuss implications for fisheries science including equity of access to technical expertise and the immediate priority of increasing transparency of AI use in fisheries science, potential pitfalls, and the need for a community of practice around prompting.

## Table of contents

## 1. Introduction

Modern ecological sciences, and particularly fisheries, demand extensive technical expertise and time-consuming computational workflows[@punt2025]. Across ecology, the complexity of modelling can be one impediment to the delivery of timely and well-informed advice for policy and decision-making [@van-nes2005]. For example, stock assessment reports often cite time constraints as a barrier to conducting comprehensive analyses, with professionals struggling to complete thorough sensitivity analyses or explore multiple scenarios [@punt2025]. Environmental research relies on a broad variety of quantitative methods, which can be challenging to learn and are underrepresented in formal training programs [@touchon2016]. Moreover, quantitative environmental scientists frequently lack formal training in programming, making the creation and maintenance of computational code difficult [@hampton2017]. Creating and sharing reproducible code that is easily understood and is reuseable by others is an even greater challenge [@Culina2020]. Thus, the limited number of trained modelers with expertise in the complexities of ecological systems creates a bottleneck for environmental assessments and exacerbates challenges associated with data availability [@dichmont2025].

Large language models (LLMs) and AI agents offer a transformative opportunity to automate complex fisheries modelling workflows [@bistarlli2025]. By separating code creation from model design and data acquisition, these technologies can potentially lower the technical barriers for non-experts [@jansen2025leveraging]. For experts, LLMs are particularly valuable in streamlining routine coding tasks, enhancing efficiency, and reducing errors, thereby freeing time for higher-level model design and scientific reasoning [@Chen2023]. For instance, AI-generated code has been applied in biological pest control distribution modelling [@scheepens2024], and in the autonomous synthesis of insect repellents [@bran2024], reducing development time and errors. However, questions remain regarding the reliability and quality of AI-generated code [@wills2024]. For example, LLMs can produce fabricated information [@scheepens2024], or produce code that executes but fails to follow best scientific practice for a particular type of computational task [@hocky2022]. Thus, to effectively use LLM for coding, users need to be familiar enough with the programming language to detect anomalies in AI outputs [@cooper2024]. Evaluations of LLM output against standardized examples therefore play an important role in determing the boundaries of appropriate use. Guidelines for the performance of state-of-the LLMs are needed to inform fisheries scientists about (A) what tasks can be completed with LLMs and (B) how can LLMs best be used to facilitate fisheries modelling. 

Two trends in LLM development are rapidly expanding their applications to scientific coding and modelling. The first is the expanding context window of LLMs. The context window is the amount of information (measured as tokens) that an LLM can process in a single chat thread. The first release of chatGPT had a context window of XXXX tokens, (approximately XXX words), whereas GPT5 has a window of XXX tokens (Ref). This increase means LLMs can follow much more complex and detailed sets of instructions to successfully complete a task (ref?). 

The second trend is increasingly sophisticated AI assistant software (Fig. 1a) and AI agent software (Fig. 1b) [@ferrag2025]. [Cut this section] ------- Unlike simple chatbots that can only respond in text, assistants and agents are given access to additional software tools. These tools are enabled by a standardized language of tool calls (known as Model Context Protocol). Simple agents have tools for reading files, writing to files and running terminal commands, including creating new files and running R program scripts. More sophisticated tools include the ability to connect to and navigate in web browsers, conduct web searches and access remotely hosted databases. ----- Combining tools with LLMs allows autonomous feedback loops between code generation, error checking, interpretation and then progression to the next task (Fig. 1b). For example, the Roo Code agent (ref) considers a user's initial prompt and generates its own 'to do list'. It then works through this checklist, solving bugs as it goes until it has completed the to do list. These feedback loops improve accuracy of task completion many times [@Bistarelli; @jansen2025leveraging]. The prompts required to initiate an agent and have it successfully complete a task are long, detailed and specific sets of instructions, also called specification sheets to differentiate them from the typical 1-2 sentence prompts people use with chatbots [@ma2024](still appropriate ref?). Agents have shown incredible performance for computer programming (ref). Bespoke agents that are customized to specific ecological modeling problems are also showing promise [@scheepens2024; @Spillias2025ecosystemmodel]. What is not clear is off-the-shelf agent software, that was developed for software programming, can be applied to solve technical and highly specialized research problems. 

![](Shared/Figure1.png)

**Figure 1** Workflows for a researcher conducting an analysis using (a) a normal workflow augmented by an AI coding assistant and (b) an analysis automated with an AI agent.

Here we test whether AI agents can transform specification sheets into reliable fisheries models by examining three increasingly complex tasks: (1) von Bertalanffy growth curve fitting, (2) generalized linear modeling of fish-habitat relationships, and (3) yield per recruit analysis. We evaluate performance on these tasks from the perspective of an expert user, whom knows what they want to do, but wants to automate the creation of code and writing.  We conduct detailed analysis of the generated responses to better understand the strengths and limitations of the AI agent. We used the Roo Code agent software and compare two different LLMs. Roo code was developed for autonomous computer programming, with primarily commercial software development in mind, so its not immediately apparent that it will work for technical science questions. We are also testing the agents ability to complete a task with the R program, which is popular with environmental statisticians, but for which there is much more limited training code available than common languages like C. Fisheries science may present a special challenge to agents, because of the relatively limited amount of training data available to LLMs relative to topics such as game programming, and that the most popular agents are calibrated for generic computer programming in common languages, not technical modelling in R. 

## 2. Methods

### Overview

We developed comprehensive specification sheets for each fisheries modelling task. These detailed instruction sets were designed to guide AI agents through complex analytical workflows while minimizing ambiguity and potential errors. For each case-study we tested the agent with two LLMs, one that was relatively cheap and open-source and one that is commercial and relatively expensive. We ran an autonomous agent for each LLM by case-study combination 10 times, to allow for non-determinism in LLM responses. Each run of the agent was then evaluated using a standarized rubric that scored the results to quantitative and semi-quantitative criteria, as was appropriate to each case-study. We then summarized the scores across the 10 replicates with accuracy, reliability and aptitude statistics. 

### Experimental Design

Our experimental approach was to test two LLMs (Claude Sonnet 4.0 and Kimi K2) against each case-study. Each case-study by LLM combination was replicated 10 times to account for the inherent randomness in AI-generated outputs [@yuan2025fp32deathchallengessolutions]. Ten replicates is widely considered sufficient to capture response variability in computational studies [@laban2025llms]. Each replicate was run independently in different chat threads and different VScode sessions, to avoid replicates sharing information via the cache. Note that Roo Code has no inbuilt memory feature that persists across sessions, so later replicates could not inadvertently learn from the results of earlier replicates. We then were able to calculate quality scores for the agent responses across the replicate runs for each of our criteria.

### Evaluation statistics 

Each case-study had different criteria for evaluation that were appropriate to the task. These included quantitative and semi-quantitative criteria. For quantitative criteria we simply scored each replicate 0/1 on whether it achieved the correct answer or not (with an error tolerance of 5%). Note that quantitative evaluation statistics, like the root mean squared error, were not useful because the agents tended to either get the correct answer, be off by orders of magnitude, or be unable to provide an answer at all. We scored semi-quantitative criteria on ordinal scales, then normalized them to a range of 0-1 for further analysis. 

The quantitive scores were then summarized across replicates with three performance indicators: Accuracy, Aptitude, and Consistency [@laban2025llms]. Accuracy was defined as the mean of the scores across replicates. Let ( \bar{S}\_j ) be the mean normalized score for question ( j ) (averaged across replicates), and let there be ( m ) questions:

$$
\text{Accuracy} = \frac{1}{m} \sum_{j=1}^{m} \bar{S}_j
$$

Criteria were further aggregated into groups of criteria that were of similar types. Aptitude and reliability were then calculated by these groups. Aptitude was the 90th percentile of the criteria-level means:

$$
\text{Aptitude} = \text{Quantile}_{0.9}(\{\bar{S}_j\}_{j=1}^m)
$$

Reliability was  one minus the difference between the 90th and 10th percentiles of the criteria-level means: 
$$
\text{Reliability} = 1 - \left( \text{Quantile}_{0.9}(\{\bar{S}_j\}_{j=1}^m) - \text{Quantile}_{0.1}(\{\bar{S}_j\}_{j=1}^m) \right)
$$

where ( \text{Quantile}\_p ) denotes the ( p )-th quantile of the set of question-level means.

### Large language models 

For LLMs we chose to test Claude Sonnet 4.0 and Kimi K2, the latest versions of these models available at the time of the study. Claude Sonnet 4.0 is well recognized as an industry leader for programming and reasoning tasks [ref]. Roo Code has been extensively developed with Sonnet in mind, so is likely also optimized for Sonnet's quirks. Other popular choices are the openAI models (e.g. GPT 5.0), but our experience is that they perform slightly less consistently for R code than Sonnet.

We also ran the agent with Kimi K2 as the LLM. Kimi K2 is a much cheaper alternative to Sonnet, but which has also shown industry leading coding abilities [ref]. Kimi K2 was built with tool-use in mind, so we tested it here to see if it can obtain similar results to Sonnet at a lower cost. 

### Agent software 

There are a growing number of agent software platforms. Here we test the Roo Code agent software. At the time of writing Roo Code was the number two ranked software by its user base [ref]. We picked this software because it allows greater customisation than the number one ranked software (Cline), so could be more useful for bespoke scientific applications.

We utilized Roo Code agentic AI software (v3.25.1, VS Code extension v1.103.0) in 'Code' mode. The LLMs were accessed via the OpenRouter API. The software was configured with auto-approval for tasks including: - File read/write operations - API request retries - Model context protocol interactions - Sub-task creation - Code execution - Self-questioning and todo list management

Manual intervention was required only for terminal commands, which were carefully vetted for safety before approval. After each task, Roo Code initiated an 'attempt_completion' protocol to finalize the agent's interactions. We then captured comprehensive metadata, including: the chat history, token input and output, estimated computational cost. 

### Case-studies

We followed similar guidelines for creating specification sheets for each of the three case-studies. These specifications provided detailed instructions for each modelling task. These documents included: - Introduction and context - Precise aims of the analysis - Detailed data methodology - Comprehensive analysis methodology - Explicit instructions for the AI agent - Technical context and constraints - Workflow description - Required directory structure - Metadata specifications. We drew best practice prompting strategies to write these instructions, including using examples and specifying a consistent structure and format for outputs [@brownLLMecology].

The specification sheets drew on fisheries and ecology text books [@haddon; @ogle2018; @zuur2009mixed] to ensure statistical and methodological accuracy. By providing all instructions upfront, we followed recent recommendations that AI systems perform more reliably with a single comprehensive initial prompt [@laban2025llms].

The specifications sheets were developed from scratch based on what our references considered to be best practice for each modelling task. We then ran some pilot tests with the agent to ensure specification sheet completeness. Minor updates were made to the specification sheets to ensure that the output was generated in a standardized way (e.g. more specific instructions for writing parameters into a csv file). The final specification sheets were then kept constant for all replications in the evaluation. 

The full spec sheets and marking rubrics are provided in the supplemental material. 

#### Von Bertalanffy Growth Model Test Case

Our simplest test for the agents was to fit a von Bertalanffy growth model to some size at age data. The von Bertalanffy was chosen because it is a common in fisheries science but rare in other disciplines and fitting it requires use of non-linear statistical algorithms [@ogle2018]. Thus, this task involves nuanced statistical challenges, particularly in parameter estimation and confidence interval calculation.

We simulated a single size-at-age dataset with known growth parameters based on *Bolbometopon muricatum* [@taylor2018parrotfish]. The AI agents were tasked with: Estimating VB growth function parameters, calculating confidence intervals via bootstrapping, predicting fish size at a specific age (5 years) and generating 95% confidence intervals for predictions of size.

The specification sheet had detailed instructions for this task, including instructions for using the `nls()` function to fit the von Bertalanffy, the growth model equation and to use the `boot` package for bootstrapping (supplementary material XXX, [@ogle2018]). We also tested a second smple specification sheet, which we tested with Sonnet because of its near 100% accuracy on all tasks. This second sheet did not include any equations or recommendations for R functions. 

We instructed the replicate agents to complete a csv file of parameter values. The agent was given an empty csv file that had only headings, such that it provided results to us in a standardized way. We then scored the responses for accuracy on each parameter and confidence interval. Each replicate and parameter estimate was scored a `1` if it was within 5% of our our estimated value.

#### Generalized linear model of fish habitat

Our second task was to test the relationship between abundance of a juvenile *Bolbometopon muricatum* (a large coral reef fish) and cover of two different types of coral. We used data from an existing study that found coral loss was impacting juveniles of a threatened fish species [@hamilton2017]. Note that past studies on this data used different methods, so the LLMs could not been trained on this exact analysis for this exact data. Further, conversations with the LLMs used here suggested they had no knowledge of the @hamilton2017 study specifically. 

The study design included 49 surveys of coral reefs in the Kia region of the Solomon Islands. At each site juvenile were counted by divers on five by 50m long transects. At the same transects benthic cover (percent) of different habitat types was also recorded using the point intersect transect method. For this analysis transects were aggregated to produce one value per site. We summed the fish counts. We took averages over the transects for two benthic habitat types: branching coral and soft coral.

We asked the AI agents to answer four questions:

1.  Does fish abundance depend on branching coral cover?
2.  What is the direction and strength of the relationship between fish abundance and branching coral cover?
3.  Does fish abundance depend on soft coral cover?
4.  What is the direction and strength of the relationship between fish abundance and branching coral cover?

The analysis aims, meta-data and approach were all completely described in the specification sheet, including the specific R functions and packages to use. We asked the agent to generate two reports, a report with model verification figures and interpretation, and a second report that had a complete write-up of the methods, results and discussion. 

One author (CJB) then manually marked the reports on criteria for:
Interpretation. Were statistics correctly interpreted, did the agent come to the correct conclusion? 
Completeness. Did the agent complete all the tasks we requested?
Technical implementation: Did the agent create the figures we requested, and did the scripts it write all run without errors (for some replicates the agent wrote the scripts but did no run them)

We also included two criteria for 'bonus points'. The first was for whether each agent looked for and identified confounding among the covariates in the dataset. We deliberately left this instruction out of the specification sheets to test whether the agents would follow statistical best practice and look at confounding. The second bonus points were for additional insights in the write-up of the results. 

There were many replicates where the agents wrote the code but did not execute the code to complete the figures and render the report as a document from the source code (rmarkdown or Rmd format). Therefore, we used this order of priority for awarding marks: We used the written reports as the primary source of information. If an Rmd file was created, but not rendered, we manually rendered it. If the Rmd did not render due to an error we read the  the Rmd file. If there was no Rmd file, or the Rmd file was missing key sections, we looked to see if there were saved figures and at the code. 

#### Yield per recruit analysis

The yield per recruit (YPR) analysis was our most complex fisheries modelling task, designed to test the agents' ability to integrate multiple biological parameters and population models. We tasked the AI agents with performing a YPR analysis for *Bolbometopon muricatum* (bumphead parrotfish) to evaluate the effects of different size limits on fishery yield [@taylor2018parrotfish; @haddon].

The agents were required to calculate YPR curves for four management scenarios: a baseline with no size restrictions and three size limit scenarios (400 mm, 500 mm, and 600 mm minimum length). For each scenario, the agents needed to determine three critical fisheries reference points: Fmax (fishing mortality producing maximum yield), Ymax (maximum yield per recruit), and F01 (the more conservative reference point where the yield curve slope equals 10% of the slope at the origin).

The specification sheet provided the biological parameters including von Bertalanffy growth parameters (Linf = 1070 mm, K = 0.15 per year, t0 = -0.074 years), natural mortality (M = 0.169 per year), length-weight relationship coefficients (a = 1.168 × 10^(-6), b = 3.409), and selectivity parameters (delta = 0.001). Maximum age was set at 29 years with initial recruitment of 1000 individuals.

The agents were instructed to implement a multi-step analytical workflow: (1) model growth using the von Bertalanffy equation, (2) convert length-at-age to weight-at-age using allometric relationships, (3) calculate age at 50% selectivity (A50) for each size limit using the inverse von Bertalanffy equation, (4) model fishing selectivity using logistic functions, (5) apply population dynamics using the Baranov catch equation incorporating both fishing and natural mortality, (6) calculate yield per recruit across a range of fishing mortalities, and (7) estimate reference points from the resulting yield curves.

The specification sheet included all necessary equations formatted as R functions: the von Bertalanffy growth equation, its inverse form for calculating age at length, length-weight conversion, logistic selectivity functions, and the Baranov catch equation for calculating catch given mortality rates. Agents were explicitly instructed to exclude mortality from the first age class and to use steep selectivity curves (small delta values) for size limit scenarios.

Expected deliverables included a completed CSV file containing all calculated reference points and yield curve plots comparing all four management scenarios. 

## 3. Results

### VBF

The the Sonnet LLM with the full specification completed the von Bertlanaffy task with perfect accuracy across all 10 replicates (fig. 2). The Kimi LLM estimated most parameters with more than 80% accurate. However, we note that Kimi's output needed some cleaning before we could produce summary results. In particular, the LLM used names for the 'length at age 5' parameter that were inconsistent with the convention we proposed in the spec.

The chat logs revealed that in two replicates Kimi made simple syntax errors in the R code, resulting in a script that did not complete the task error free. It thus manually entered parameter values into the parameter table, putting those values in the wrong positions.

The Sonnet LLM also performed close to 100% accuracy for all parameters when using the simpler specification that had no instructions about algorithms to use for fitting the von Bertalanffy (fig 2a). It scored 100% accuracy for parameters K and Linf and the estimate of the mean length at age 5. Accuracy was lower for the confidence intervals for length at age 5. However, when reviewing the R code it created we noticed it commonly used the t distribution for estimating confidence intervals, compared to our solution that used bootstrapping. Hence the confidence intervals were greater than 5% different to ours in 30-40% of cases, but still estimated with a valid algorithm. Likewise, the values for t0 were often \>5% but \<10% different to our values.

![](Shared/Outputs/fig2-vbf_Claude-Kimi-simple.png)

**Figure 2** Results for the von Bertalanffy case-study. (a) Mean accuracy for the two LLMs and each parameter estimate. (b) Summary of mean accuracy, aptitude and reliability for the two LLMs parameters.

### GLM

The Sonnet LLM was able to consistently complete the full task, from data import, data wrangling, statistical analysis, verification of analysis and report writing (fig. 3). It always conducted the model simplification using likelihood ratios (as requested) and used the appropriate p-value to interpret the results in the written reports. Inconsistencies for the Sonnet LLM were that it did not always produce diagnostic plots that were relevant for count data and it did not always provide interpretation of the diagnostic plots. Strangely, despite its high accuracy on all other tasks, the Sonnet LLM was almost incapable of following the simple instruction to use a log scale on the y-axis of predictive plots.

The Kimi LLM did not perform reliably or accurately on the GLM task (fig 3). It frequently failed to complete reports (no report created 30% of the time) or did not follow instructions precisely. It often created code for the likelihood ratio test, but then used a different or made-up p-value in reporting (fig 3a). In some cases the LLM generated code for plots that executed successfully when we tested it, but did not complete the full reporting task. We found cases of fabrication in the chat logs for the Kimi agent. The Kimi agent often stated it had completed the reports, although it had not done so. The only criteria where the Kimi LLM outperformed the Sonnet LLM was on using the correct scale for the predictive plots (fig 3a).

The Kimi LLM had high aptitude for completeness (92%) and technical implementation (96%) (fig 3b), indicating that it could perform well at these types of tasks, just not reliably.

Neither the Kimi or the Sonnet LLM performed well on the 'bonus points' tasks: additional important considerations that we did not explicitly request in the GLM spec. Sonnet occasionally made further interpretation of the data that we judged as valuable contributions. Neither model ever picked up on confounding between the two covariates and neither model suggested that confounding as something that should be tested for.

![](Shared/Outputs/figure-3.png)

**Figure 3** Results for the GLM case-study. (a) Mean accuracy for the two LLMs and each criteria from the rubric. (b) Summary of mean accuracy, aptitude and reliability for the two LLMs by criteria types.

### YPR

The Sonnet LLM demonstrated moderate performance on the yield per recruit (YPR) analysis. Sonnet consistently completed the assigned tasks, generating executable code, producing plots, and populating parameter tables as specified. In contrast, the Kimi LLM was less reliable, frequently failing to complete the workflow. Kimi often produced code that contained errors or was not executed, resulting in incomplete outputs.

Sonnet generally produced accurate results for most parameters. However, a recurring issue was the incorrect application of fishing and natural mortality rates, particularly the misallocation of mortality to the first age class despite explicit instructions to exclude it. This error substantially affected the estimation of maximum yield (Ymax), leading to lower accuracy for this parameter. Kimi's performance was more variable, with occasional high aptitude scores but frequent failures to complete the analysis or generate valid outputs.

Overall, Sonnet provided robust and reproducible results for most aspects of the YPR analysis, but both LLMs exhibited limitations in handling nuanced methodological details, underscoring the need for expert oversight and careful prompt engineering.

![](Shared/Outputs/figure-4-YPR-quant_and_qual.png)

**Figure 4** Results for the yield-per-recruit case-study. (a) Mean accuracy for the two LLMs for criteria related to technical implementation of the code, (b) mean accuracy for criteria related to the quantitative results.

### Cost and accuracy

We compared the average accuracy scores against total cost for the quantitative evaluations for the VBF and YPR case-studies (fig. 5). There was a pattern of Kimi's accuracy being lower but also cheaper than Sonnet's accuracy. However, within a model there was no trend of higher cost solutions tending to have lower accuracy. For example, Kimi failed two times to complete the VBF task, in both of those times it got stuck in an endless tool calling loop, wasted tokens and never found a solution (fig. 5a). Sonnet's most expensive solutions for the YPR also tended to be less accurate. 

![](Shared/Outputs/figure-5-cost-accuracy.png)

## 4. Discussion

This study demonstrates that agentic AI systems can successfully automate common fisheries modelling workflows, but performance varies significantly between different large language models. Our evaluation of AI agents across three increasingly complex fisheries modelling tasks—von Bertalanffy growth fitting, generalized linear modelling, and yield per recruit analysis—reveals that Claude Sonnet 4.0 consistently achieved high accuracy (near 100% for simple tasks, moderate performance for complex tasks), while Kimi K2 showed variable and often unreliable performance. This represents the first systematic evaluation of general-purpose AI agents for fisheries modeling tasks, establishing baseline performance expectations and demonstrating the feasibility of automating routine analytical workflows in fisheries science.

The performance patterns revealed distinct capabilities and limitations across different types of modeling tasks. Sonnet demonstrated consistent high performance across the complexity gradient from simple parameter estimation to complex population modeling, maintaining reliable task completion and accurate statistical interpretation. In contrast, Kimi exhibited significant variability, with particular weaknesses in complex analytical workflows where it frequently failed to complete tasks or generated plausible but incorrect outputs—a form of hallucination particularly concerning in scientific applications. Our results highlight the importance of detailed specification sheets over simple prompts, with comprehensive instructions improving consistency across replicates. However, even detailed prompts could not overcome fundamental model limitations, suggesting that performance is largely determined by the underlying LLM capabilities rather than prompt engineering alone.

These findings align with broader trends in AI-assisted scientific computing while revealing domain-specific challenges for fisheries applications [@bistarlli2025; @scheepens2024; @bran2024]. Unlike general programming tasks where LLMs benefit from extensive training data, fisheries modeling represents a specialized domain with limited publicly available code and methodological examples. This scarcity likely contributes to the variable performance we observed, particularly for complex tasks requiring integration of multiple biological parameters and statistical methods. The expanding context windows and tool integration capabilities of modern LLMs [@ferrag2025] enable the complex workflows we tested, but domain-specific expertise remains challenging to automate. Our results suggest that while AI agents can successfully execute pre-defined analytical protocols, they cannot replicate the expert judgment required for methodological decisions such as identifying confounding variables or selecting appropriate diagnostic procedures.

Several important limitations must be considered when interpreting these results and their applicability to real-world fisheries applications. Our evaluation was limited to three modeling tasks using simulated or controlled datasets, which may not capture the complexity and messiness of real-world fisheries data. We tested only one agent platform (Roo Code), and results may not generalize to other AI systems or configurations. The evaluation rubrics were designed by the study authors, potentially introducing bias toward specific analytical approaches. Additionally, LLM capabilities are evolving rapidly, meaning these performance benchmarks may quickly become outdated. These limitations do not undermine our core findings because they establish a baseline for performance evaluation under controlled conditions, allowing systematic comparison between models and providing a foundation for future evaluations with more complex, real-world scenarios.

The demonstrated capabilities of AI agents have significant implications for democratizing access to fisheries modeling expertise and accelerating assessment timelines. By automating routine coding and analytical tasks, AI agents could address the well-documented bottleneck of limited trained modelers in fisheries science [@punt2025; @dichmont2025]. This is particularly relevant for data-poor fisheries management contexts where technical expertise may be scarce but basic modeling approaches could inform management decisions. The cost-effectiveness trade-offs between different LLMs (Sonnet's high accuracy versus Kimi's lower cost) suggest that different models may be appropriate for different applications—high-stakes assessments may justify premium models, while exploratory analyses or educational applications could utilize more affordable alternatives. However, the need for expert oversight and verification protocols remains paramount, as none of the tested agents demonstrated the expert judgment required for independent statistical analysis.

Successful implementation of AI agents in fisheries science requires establishing best practices for prompt engineering, output verification, and responsible use. Our results demonstrate that comprehensive specification sheets significantly improve performance consistency, suggesting that developing standardized templates for common fisheries modeling tasks could enhance reliability. Expert oversight remains essential—while agents can automate code generation and execution, human expertise is still required for methodological decisions, result interpretation, and quality assurance. The fisheries community needs to develop transparent protocols for documenting AI use in scientific publications, including specification of which tasks were automated and how outputs were verified. Future research priorities should include testing agents on real-world datasets with missing data and methodological complexities, expanding evaluations to other modeling approaches (e.g., stock assessments, ecosystem models), and developing fisheries-specific AI tools trained on domain-relevant data.

Agentic AI systems represent a promising tool for fisheries science, offering the potential to democratize access to technical expertise and accelerate the delivery of scientific advice for management. However, their successful integration requires careful consideration of their capabilities and limitations. While our results demonstrate that AI agents can reliably execute pre-defined analytical workflows, they cannot replace expert judgment in statistical analysis and interpretation. The most productive path forward likely involves human-AI collaboration, where experts design analytical frameworks and interpret results while agents automate the routine coding and computational tasks that currently consume substantial time and resources.

## 5. Conclusion

Next steps 
Full stock assessment
Multi stage modelling (VBF fitting, age structure, pop dynamics etc... )
But even so, we've shown that agents can automate many stages of the analysis, potentially speeidng up work for fisheries scientists. 

## Acknowledgements

## Data availability

-   plus link to repository

## Conflict of Interest

## References

## Supplemental material

## Tables

-   All abbreviations must be defined in footnotes. Footnote symbols: †, ‡, §, ¶, should be used (in that order) and \*, \*\*,\*\*\* should be reserved for P-values
-   **TABLE 1.** xxx

## Figure Legends

-   Figures and supporting info should be supplied as separate files
    -   [electronic_artwork_guidelines.pdf](https://media.wiley.com/assets/7323/92/electronic_artwork_guidelines.pdf)
    -   In-text Figures: **CAPTION 1 (A)** xxx
    -   Supporting information Figures: **Table S1** xxx; **Appendix S1**
-   Include definitions of any symbols used and define/explain all abbreviations and units of measurement