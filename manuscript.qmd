---
title: "Automating fisheries modelling with agentic AI"
bibliography: references.bib
# csl: oneearth.csl
editor: source
format: docx
    #reference-doc: template.docx
---

Journal: Fish and Fisheries

## Abstract

-   250 words

-   6 keywords - alphabetical order

\[re-write this as its for expert users as well. \] Fisheries modelling can take years of specialist training to learn. Agentic AI systems automate complex computing programming workflows and could lower the technical barrier for fisheries modelling. However, questions remain about the quality of AI derived models. Here we test whether agentic AI can write computer code to complete three common types of fisheries models. We test an agentic AI system (Roo Code) on its ability to complete three fisheries modelling workflows, from data analysis to report write-up: (1) von-Bertalanffy parameter estimation, (2) generalized linear modelling of fish-habitat relationships, (3) yield per recruit analysis. We use replicate prompts and a rubric to evaluate the AI generated reports. We find that... We show how careful prompting of the AI system can deliver high quality reports for fisheries modelling problems. Our results show that agentic AI systems can already complete complex fisheries workflows, particularly if users provide context-rich prompts. We discuss implications for fisheries science including equity of access to technical expertise and the immediate priority of increasing transparency of AI use in fisheries science, potential pitfalls, and the need for a community of practice around prompting.

## Table of contents

## 1. Introduction

Modern ecological sciences, and particularly fisheries, demand extensive technical expertise and time-consuming computational workflows[@punt2025]. Across ecology, the complexity of modelling can be one impediment to the delivery of timely and well-informed advice for policy and decision-making [@van-nes2005]. For example, stock assessment reports often cite time constraints as a barrier to conducting comprehensive analyses, with professionals struggling to complete thorough sensitivity analyses or explore multiple scenarios [@punt2025]. Environmental research relies on a broad variety of quantitative methods, which can be challenging to learn and are underrepresented in formal training programs [@touchon2016]. Moreover, quantitative environmental scientists frequently lack formal training in programming, making the creation and maintenance of computational code difficult [@hampton2017]. Creating and sharing reproducible code that is easily understood and is reuseable by others is an even greater challenge [@Culina2020]. Thus, the limited number of trained modelers with expertise in the complexities of ecological systems creates a bottleneck for environmental assessments and exacerbates challenges associated with data availability [@dichmont2025].

Large language models (LLMs) and AI agents offer a transformative opportunity to automate complex fisheries modelling workflows [@bistarlli2025]. By separating code creation from model design and data acquisition, these technologies can potentially lower the technical barriers for non-experts [@jansen2025leveraging]. For experts, LLMs are particularly valuable in streamlining routine coding tasks, enhancing efficiency, and reducing errors, thereby freeing time for higher-level model design and scientific reasoning [@Chen2023]. For instance, AI-generated code has been applied in biological pest control distribution modelling [@scheepens2024], and in the autonomous synthesis of insect repellents [@bran2024], reducing development time and errors. However, questions remain regarding the reliability and quality of AI-generated code [@wills2024]. For example, LLMs can produce fabricated information [@scheepens2024], or produce code that executes but fails to follow best scientific practice for a particular type of computational task [@hocky2022]. Thus, to effectively use LLM for coding, users need to be familiar enough with the programming language to detect anomalies in AI outputs [@cooper2024].

Two trends in LLM development are rapidly expanding their applications to scientific coding and modelling. The first is the expanding context window of LLMs. The context window is the amount of information (measured as tokens) that an LLM can process in a single chat thread. The first release of chatGPT had a context window of XXXX tokens, (approximately XXX words), whereas GPT5 has a window of XXX tokens (Ref). This increase means LLMs can follow much more complex and detailed sets of instructions to successfully complete a task (ref?). 

The second trend is increasingly sophisticated AI assistant software (Fig. 1a) and AI agent software (Fig. 1b) [@ferrag2025]. [Cut this section] ------- Unlike simple chatbots that can only respond in text, assistants and agents are given access to additional software tools. These tools are enabled by a standardized language of tool calls (known as Model Context Protocol). Simple agents have tools for reading files, writing to files and running terminal commands, including creating new files and running R program scripts. More sophisticated tools include the ability to connect to and navigate in web browsers, conduct web searches and access remotely hosted databases. ----- Combining tools with LLMs allows autonomous feedback loops between code generation, error checking, interpretation and then progression to the next task (Fig. 1b). For example, the Roo Code agent (ref) considers a user's initial prompt and generates its own 'to do list'. It then works through this checklist, solving bugs as it goes until it has completed the to do list. These feedback loops improve accuracy of task completion many times [@Bistarelli; @jansen2025leveraging]. The prompts required to initiate an agent and have it successfully complete a task are long, detailed and specific sets of instructions, also called specification sheets to differentiate them from the typical 1-2 sentence prompts people use with chatbots [@ma2024](still appropriate ref?). Agents have shown incredible performance for computer programming (ref). Bespoke agents that are customized to specific ecological modeling problems are also showing promise [@scheepens2024; @Spillias2025ecosystemmodel]. What is not clear is off-the-shelf agent software, that was developed for software programming, can be applied to solve technical and highly specialized research problems. 

![](Shared/Figure1.png)

**Figure 1** Workflows for a researcher conducting an analysis using (a) a normal workflow augmented by an AI coding assistant and (b) an analysis automated with an AI agent.

Here we test whether AI agents can transform specification sheets into reliable fisheries models by examining three increasingly complex tasks: (1) von Bertalanffy growth curve fitting, (2) generalized linear modeling of fish-habitat relationships, and (3) yield per recruit analysis. We evaluate performance on these tasks from the perspective of an expert user, whom knows what they want to do, but wants to automate the creation of code and writing.  We conduct detailed analysis of the generated responses to better understand the strengths and limitations of the AI agent. We used the Roo Code agent software and compare two different LLMs. Roo code was developed for autonomous computer programming, with primarily commercial software development in mind, so its not immediately apparent that it will work for technical science questions. We are also testing the agents ability to complete a task with the R program, which is popular with environmental statisticians, but for which there is much more limited training code available than common languages like C. Fisheries science may present a special challenge to agents, because of the relatively limited amount of training data available to LLMs relative to topics such as game programming, and that the most popular agents are calibrated for generic computer programming in common languages, not technical modelling in R. 

## 2. Methods

### Overview

We developed comprehensive specification sheets for each fisheries modelling task. These detailed instruction sets were designed to guide AI agents through complex analytical workflows while minimizing ambiguity and potential errors. For each case-study we tested the agent with two LLMs, one that was relatively cheap and open-source and one that is commercial and relatively expensive. We ran an autonomous agent for each LLM by case-study combination 10 times, to allow for non-determinism in LLM responses. Each run of the agent was then evaluated using a standarized rubric that scored the results to quantitative and semi-quantitative criteria, as was appropriate to each case-study. We then summarized the scores across the 10 replicates with accuracy, reliability and aptitude statistics. 

### Experimental Design

Our experimental approach was to test two LLMs (Claude Sonnet 4.0 and Kimi K2) against each case-study. Each case-study by LLM combination was replicated 10 times to account for the inherent randomness in AI-generated outputs [@yuan2025fp32deathchallengessolutions]. Ten replicates is widely considered sufficient to capture response variability in computational studies [@laban2025llms]. Each replicate was run independently in different chat threads and different VScode sessions, to avoid replicates sharing information via the cache. Note that Roo Code has no inbuilt memory feature that persists across sessions, so later replicates could not inadvertently learn from the results of earlier replicates. We then were able to calculate quality scores for the agent responses across the replicate runs for each of our criteria.

### Evaluation statistics 

Each case-study had different criteria for evaluation that were appropriate to the task. These included quantitative and semi-quantitative criteria. For quantitative criteria we simply scored each replicate 0/1 on whether it achieved the correct answer or not (with an error tolerance of 5%). Note that quantitative evaluation statistics, like the root mean squared error, were not useful because the agents tended to either get the correct answer, be off by orders of magnitude, or be unable to provide an answer at all. We scored semi-quantitative criteria on ordinal scales, then normalized them to a range of 0-1 for further analysis. 

The quantitive scores were then summarized across replicates with three performance indicators: Accuracy, Aptitude, and Consistency [@laban2025llms]. Accuracy was defined as the mean of the scores across replicates. Let ( \bar{S}\_j ) be the mean normalized score for question ( j ) (averaged across replicates), and let there be ( m ) questions:

$$
\text{Accuracy} = \frac{1}{m} \sum_{j=1}^{m} \bar{S}_j
$$

Criteria were further aggregated into groups of criteria that were of similar types. Aptitude and reliability were then calculated by these groups. Aptitude was the 90th percentile of the criteria-level means:

$$
\text{Aptitude} = \text{Quantile}_{0.9}(\{\bar{S}_j\}_{j=1}^m)
$$

Reliability was  one minus the difference between the 90th and 10th percentiles of the criteria-level means: 
$$
\text{Reliability} = 1 - \left( \text{Quantile}_{0.9}(\{\bar{S}_j\}_{j=1}^m) - \text{Quantile}_{0.1}(\{\bar{S}_j\}_{j=1}^m) \right)
$$

where ( \text{Quantile}\_p ) denotes the ( p )-th quantile of the set of question-level means.

### Large language models 

For LLMs we chose to test Claude Sonnet 4.0 and Kimi K2, the latest versions of these models available at the time of the study. Claude Sonnet 4.0 is well recognized as an industry leader for programming and reasoning tasks [ref]. Roo Code has been extensively developed with Sonnet in mind, so is likely also optimized for Sonnet's quirks. Other popular choices are the openAI models (e.g. GPT 5.0), but our experience is that they perform slightly less consistently for R code than Sonnet.

We also ran the agent with Kimi K2 as the LLM. Kimi K2 is a much cheaper alternative to Sonnet, but which has also shown industry leading coding abilities [ref]. Kimi K2 was built with tool-use in mind, so we tested it here to see if it can obtain similar results to Sonnet at a lower cost. 

### Agent software 

There are a growing number of agent software platforms. Here we test the Roo Code agent software. At the time of writing Roo Code was the number two ranked software by its user base [ref]. We picked this software because it allows greater customisation than the number one ranked software (Cline), so could be more useful for bespoke scientific applications.

We utilized Roo Code agentic AI software (v3.25.1, VS Code extension v1.103.0) in 'Code' mode. The LLMs were accessed via the OpenRouter API. The software was configured with auto-approval for tasks including: - File read/write operations - API request retries - Model context protocol interactions - Sub-task creation - Code execution - Self-questioning and todo list management

Manual intervention was required only for terminal commands, which were carefully vetted for safety before approval. After each task, Roo Code initiated an 'attempt_completion' protocol to finalize the agent's interactions. We then captured comprehensive metadata, including: the chat history, token input and output, estimated computational cost. 

### Case-studies

We followed similar guidelines for creating specification sheets for each of the three case-studies. These specifications provided detailed instructions for each modelling task. These documents included: - Introduction and context - Precise aims of the analysis - Detailed data methodology - Comprehensive analysis methodology - Explicit instructions for the AI agent - Technical context and constraints - Workflow description - Required directory structure - Metadata specifications. We drew best practice prompting strategies to write these instructions, including using examples and specifying a consistent structure and format for outputs [@brownLLMecology].

The specification sheets drew on fisheries and ecology text books [@haddon; @ogle2018; @zuur2009mixed] to ensure statistical and methodological accuracy. By providing all instructions upfront, we followed recent recommendations that AI systems perform more reliably with a single comprehensive initial prompt [@laban2025llms].

The specifications sheets were developed from scratch based on what our references considered to be best practice for each modelling task. We then ran some pilot tests with the agent to ensure specification sheet completeness. Minor updates were made to the specification sheets to ensure that the output was generated in a standardized way (e.g. more specific instructions for writing parameters into a csv file). The final specification sheets were then kept constant for all replications in the evaluation. 

The full spec sheets and marking rubrics are provided in the supplemental material. 

#### Von Bertalanffy Growth Model Test Case

Our simplest test for the agents was to fit a von Bertalanffy growth model to some size at age data. The von Bertalanffy was chosen because it is a common in fisheries science but rare in other disciplines and fitting it requires use of non-linear statistical algorithms [@ogle2018]. Thus, this task involves nuanced statistical challenges, particularly in parameter estimation and confidence interval calculation.

We simulated a single size-at-age dataset with known growth parameters based on *Bolbometopon muricatum* [@taylor2018parrotfish]. The AI agents were tasked with: Estimating VB growth function parameters, calculating confidence intervals via bootstrapping, predicting fish size at a specific age (5 years) and generating 95% confidence intervals for predictions of size.

The specification sheet had detailed instructions for this task, including instructions for using the `nls()` function to fit the von Bertalanffy, the growth model equation and to use the `boot` package for bootstrapping (supplementary material XXX, [@ogle2018]).

We instructed the replicate agents to complete a csv file of parameter values. The agent was given an empty csv file that had only headings, such that it provided results to us in a standardized way. We then scored the responses for accuracy on each parameter and confidence interval. Each replicate and parameter estimate was scored a `1` if it was within 5% of our our estimated value.

### Generalized linear model of fish habitat

Our second task was to test the relationship between abundance of a juvenile *Bolbometopon muricatum* (a large coral reef fish) and cover of two different types of coral. We used data from an existing study that found coral loss was impacting juveniles of a threatened fish species [@hamilton2017]. Note that past studies on this data used different methods, so the LLMs could not been trained on this exact analysis for this exact data. Further, conversations with the LLMs used here suggested they had no knowledge of the @hamilton2017 study specifically. 

The study design included 49 surveys of coral reefs in the Kia region of the Solomon Islands. At each site juvenile were counted by divers on five by 50m long transects. At the same transects benthic cover (percent) of different habitat types was also recorded using the point intersect transect method. For this analysis transects were aggregated to produce one value per site. We summed the fish counts. We took averages over the transects for two benthic habitat types: branching coral and soft coral.

We asked the AI agents to answer four questions:

1.  Does fish abundance depend on branching coral cover?
2.  What is the direction and strength of the relationship between fish abundance and branching coral cover?
3.  Does fish abundance depend on soft coral cover?
4.  What is the direction and strength of the relationship between fish abundance and branching coral cover?

The analysis aims, meta-data and approach were all completely described in the specification sheet, including the specific R functions and packages to use. We asked the agent to generate two reports, a report with model verification figures and interpretation, and a second report that had a complete write-up of the methods, results and discussion. 

One author (CJB) then manually marked the reports on criteria for:
Interpretation. Were statistics correctly interpreted, did the agent come to the correct conclusion? 
Completeness. Did the agent complete all the tasks we requested?
Technical implementation: Did the agent create the figures we requested, and did the scripts it write all run without errors (for some replicates the agent wrote the scripts but did no run them)

We also included two criteria for 'bonus points'. The first was for whether each agent looked for and identified confounding among the covariates in the dataset. We deliberately left this instruction out of the specification sheets to test whether the agents would follow statistical best practice and look at confounding. The second bonus points were for additional insights in the write-up of the results. 

There were many replicates where the agents wrote the code but did not execute the code to complete the figures and render the report as a document from the source code (rmarkdown or Rmd format). Therefore, we used this order of priority for awarding marks: We used the written reports as the primary source of information. If an Rmd file was created, but not rendered, we manually rendered it. If the Rmd did not render due to an error we read the  the Rmd file. If there was no Rmd file, or the Rmd file was missing key sections, we looked to see if there were saved figures and at the code. 

### Yield per recruit analysis

Used parameter for *Bolbometopon muricatum* [@taylor2018parrotfish]. Based code on @haddon



## 3. Results

### VBF

The Sonnet LLM with the full spec completed the von Bertlanaffy task with perfect accuracy across all 10 replicates (fig. 2). The Kimi LLM estimated most parameters more than 80% accurate. However, we note that Kimi's output needed some cleaning before we could produce summary results. In particular, the LLM used names for the 'length at age 5' parameter that were inconsistent with the convention we proposed in the spec.

The chat logs revealed that in two replicates Kimi made simple syntax errors in the R code, resulting in a script that did not complete the task error free. It thus manually entered parameter values into the parameter table, putting those values in the wrong positions.

The Sonnet LLM also performed close to 100% accuracy for all parameters when using the simpler specification that had no instructions about algorithms to use for fitting the von Bertalanffy (fig 2a). It scored 100% accuracy for parameters K and Linf and the estimate of the mean length at age 5. Accuracy was lower for the confidence intervals for length at age 5. However, when reviewing the R code it created we noticed it commonly used the t distribution for estimating confidence intervals, compared to our solution that used bootstrapping. Hence the confidence intervals were greater than 5% different to ours in 30-40% of cases, but still estimated with a valid algorithm. Likewise, the values for t0 were often \>5% but \<10% different to our values.

![](Shared/Outputs/fig2-vbf_Claude-Kimi-simple.png)

**Figure 2** Results for the von Bertalanffy case-study. (a) Mean accuracy for the two LLMs and each parameter estimate. (b) Summary of mean accuracy, aptitude and reliability for the two LLMs parameters.

### GLM

The Sonnet LLM was able to consistently complete the full task, from data import, data wrangling, statistical analysis, verification of analysis and report writing (fig. 3). It always conducted the model simplification using likelihood ratios (as requested) and used the appropriate p-value to interpret the results in the written reports. Inconsistencies for the Sonnet LLM were that it did not always produce diagnostic plots that were relevant for count data and it did not always provide interpretation of the diagnostic plots. Strangely, despite its high accuracy on all other tasks, the Sonnet LLM was almost incapable of following the simple instruction to use a log scale on the y-axis of predictive plots.

The Kimi LLM did not perform reliably or accurately on the GLM task (fig 3). It frequently failed to complete reports (no report created 30% of the time) or did not follow instructions precisely. It often created code for the likelihood ratio test, but then used a different or made-up p-value in reporting (fig 3a). In some cases the LLM generated code for plots that executed successfully when we tested it, but did not complete the full reporting task. We found cases of fabrication in the chat logs for the Kimi agent. The Kimi agent often stated it had completed the reports, although it had not done so. The only criteria where the Kimi LLM outperformed the Sonnet LLM was on using the correct scale for the predictive plots (fig 3a).

The Kimi LLM had high aptitude for completeness (92%) and technical implementation (96%) (fig 3b), indicating that it could perform well at these types of tasks, just not reliably.

Neither the Kimi or the Sonnet LLM performed well on the 'bonus points' tasks: additional important considerations that we did not explicitly request in the GLM spec. Sonnet occasionally made further interpretation of the data that we judged as valuable contributions. Neither model ever picked up on confounding between the two covariates and neither model suggested that confounding as something that should be tested for.

![](Shared/Outputs/figure-3.png)

**Figure 3** Results for the GLM case-study. (a) Mean accuracy for the two LLMs and each criteria from the rubric. (b) Summary of mean accuracy, aptitude and reliability for the two LLMs by criteria types.

### YPR

The Sonnet LLM demonstrated moderate performance on the yield per recruit (YPR) analysis. Sonnet consistently completed the assigned tasks, generating executable code, producing plots, and populating parameter tables as specified. In contrast, the Kimi LLM was less reliable, frequently failing to complete the workflow. Kimi often produced code that contained errors or was not executed, resulting in incomplete outputs.

Sonnet generally produced accurate results for most parameters. However, a recurring issue was the incorrect application of fishing and natural mortality rates, particularly the misallocation of mortality to the first age class despite explicit instructions to exclude it. This error substantially affected the estimation of maximum yield (Ymax), leading to lower accuracy for this parameter. Kimi's performance was more variable, with occasional high aptitude scores but frequent failures to complete the analysis or generate valid outputs.

Overall, Sonnet provided robust and reproducible results for most aspects of the YPR analysis, but both LLMs exhibited limitations in handling nuanced methodological details, underscoring the need for expert oversight and careful prompt engineering.

![](Shared/Outputs/figure-4-YPR-quant_and_qual.png)

**Figure 4** Results for the yield-per-recruit case-study. (a) Mean accuracy for the two LLMs for criteria related to technical implementation of the code, (b) mean accuracy for criteria related to the quantitative results.

### Cost and accuracy

We compared the average accuracy scores against total cost for the quantitative evaluations for the VBF and YPR case-studies (fig. 5). There was a pattern of Kimi's accuracy being lower but also cheaper than Sonnet's accuracy. However, within a model there was no trend of higher cost solutions tending to have lower accuracy. For example, Kimi failed two times to complete the VBF task, in both of those times it got stuck in an endless tool calling loop, wasted tokens and never found a solution (fig. 5a). Sonnet's most expensive solutions for the YPR also tended to be less accurate. 

![](Shared/Outputs/figure-5-cost-accuracy.png)

## 4. Discussion

The analysis of Sonnet and Kimi on the VBGF, GLM and YPR models provides valuable insight into the strengths and limitations of these two LLMs in the context of fisheries management. Their performance varies when linguistically and cognitively tasked with managing a diverse range of fisheries models that vary in complexities and difficulty. Sonnet consistently outperformed Kimi in task execution, demonstrating superior performance regardless of the specificity of the prompts.

Fisheries modellers often begin without fully developed instructions, as crafting precise prompts takes time. Our findings show that well-worded instructions contribute to repeatability, with results remaining reasonable consistent across trials. However, detailed. However, detailed prompts alone do not guarantee accurate outcomes; performance is largely influenced by the inherent characteristics of each model. Moreover, all models carry intrinsic limitation that fisheries modellers must recognise to avoid introducing bias into decison-making processes. One critical issues is model hallucination - where outputs appear plausible but are unsupported by evidence. This is particularly concerning in fisheries, where accuracy is essential. Hallucination typically occurs when the model lacks sufficient evidence in its training data to support its response [@chen2025; @shan2024benchmarking]. These limitations provides insight into the strengths and weaknesses of both models when applied to complex linguistic task in fisheries analysis. Ultimately, this emphasise the need for continuous improvement and vigorous evaluation, to ensure the development of reliable and efficient LLM for fisheries application.

Kimi exhibited hallucinations during it analysis, likely due to insufficient training data to support its responses. Its tendency to generate plausible yet incorrect outputs undermines its reliability, posing a risk in fisheries applications where high accuracy is essential for decision making. Basiscally would be good as an AI aid, but not trusted to work on its own. Its work needs more correction and checking. Note that could be interation with Roo Code set-up which is optimised for Sonnet.

Cost accuracy, down rabbit holes (someone else has commented on this I think.... )

Sonnet's performance on the VBGF, GLM, and YPR models suggests that AI agents are capable of producing high-quality fisheries reports to complement those generated by human fisheries experts in decision making. Although Sonnet performed notably better than Kimi, there remains room for improvement.

Vibe coding, not for high stakes situations. Ok to try somethign out or for learning. If using agents you should check and verify all code. Often need to iterate a lot on the prompts to develop a good prompt. Also creating multiple versions and implementations of a project to compare and get ideas.

Overall we found Sonnet to be a reliable and accurate implementer of the statistical task and that it could consistently follow instructions. However, it could not be relied upon as an expert statistician - it did not consistently provide best-practice verification plots and it never suggested we check for confounding. Therefore, it is a powerful tool if used by an expert, or given clear steps on the workflow required for a statistical valid analysis.

A key contribution of this study is the evaluation of AI agents' performance across a comprehensive range of stock assessment, progressing from simple to more complex fisheries frameworks. This approach provides valuable insight into how LLMs handle increasing model complexity. The findings show promise for enhancing fisheries data analysis - particularly stock assessments - with practical applications in both data poor and data rich contexts. Ultimately, this supports more informed and sustainable decision making in fisheries management.

## 5. Conclusion

## Acknowledgements

## Data availability

-   plus link to repository

## Conflict of Interest

## References

## Supplemental material

## Tables

-   All abbreviations must be defined in footnotes. Footnote symbols: †, ‡, §, ¶, should be used (in that order) and \*, \*\*,\*\*\* should be reserved for P-values
-   **TABLE 1.** xxx

## Figure Legends

-   Figures and supporting info should be supplied as separate files
    -   [electronic_artwork_guidelines.pdf](https://media.wiley.com/assets/7323/92/electronic_artwork_guidelines.pdf)
    -   In-text Figures: **CAPTION 1 (A)** xxx
    -   Supporting information Figures: **Table S1** xxx; **Appendix S1**
-   Include definitions of any symbols used and define/explain all abbreviations and units of measurement