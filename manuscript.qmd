---
title: "Automating fisheries modelling with agentic AI"
bibliography: references.bib
# csl: oneearth.csl
editor: visual
format: docx
# format:
  # docx:
    # reference-doc: template.docx
---

Journal: Fish and Fisheries 

## Abstract

-   250 words

-   6 keywords - alphabetical order

\[re-write this as its for expert users as well. \] Fisheries modelling can take years of specialist training to learn. Agentic AI systems automate complex computing programming workflows and could lower the technical barrier for fisheries modelling. However, questions remain about the quality of AI derived models. Here we test whether agentic AI can write computer code to complete three common types of fisheries models. We test an agentic AI system (Roo Code) on its ability to complete three fisheries modelling workflows, from data analysis to report write-up: (1) von-Bertalanffy parameter estimation, (2) generalized linear modelling of fish-habitat relationships, (3) yield per recruit analysis. We use replicate prompts and a rubric to evaluate the AI generated reports. We find that... We show how careful prompting of the AI system can deliver high quality reports for fisheries modelling problems. Our results show that agentic AI systems can already complete complex fisheries workflows, particularly if users provide context-rich prompts. We discuss implications for fisheries science including equity of access to technical expertise and the immediate priority of increasing transparency of AI use in fisheries science, potential pitfalls, and the need for a community of practice around prompting.


## Table of contents

## Introduction

Fisheries modeling demands extensive technical expertise and time-consuming computational workflows. The complexity of modelling can be one impediment to the delivery of timely and well-informed advice to fishery managers. For example, stock assessment reports often cite time constraints as a barrier to conducting comprehensive analyses, with professionals struggling to complete thorough sensitivity analyses or explore multiple scenarios (insert ref to stock assessment). Fish ecology more generally relies on a broad variety of quantitative methods, which can be difficult to learn. ... stuff abotu open code in modelling... The limited number of trained modelers further constrains fisheries stock assessments, compounding issues with constraints on data availability.

Fisheries m....

Large language models (LLMs) and AI agents offer a transformative opportunity to automate complex fisheries modeling workflows. By separating code creation from model design and data acquisition, these technologies can potentially lower the technical barriers to entry. However, critical questions remain about the reliability and quality of AI-generated code.

Previous tests, primarily in the R language, have revealed significant challenges. While LLMs can generate executable code, less than 50% of generated scripts were technically accurate [@jansen2025leveraging]. Notably, code execution rates improved when AI models could self-correct [@zhu2024large]. Key insights have emerged about effective AI interaction:

-   Contextual information is crucial for making informed statistical choices
-   Providing reference code and statistical approaches guides AI agents
-   One-shot prompting with comprehensive examples often yields more reliable results than multi-turn conversations [@laban2025llms]

The development of detailed specification sheets—long, precise instruction sets written in plain English—represents a promising approach to improving AI-generated analysis. These specifications can articulate analysis requirements clearly, making them accessible to both expert and novice users.

We test whether AI agents can transform specification sheets into reliable fisheries models by examining three increasingly complex tasks: (1) von Bertalanffy growth curve fitting, (2) generalized linear modeling of fish-habitat relationships, and (3) yield per recruit analysis. We explore two prompting strategies: one representing an expert user providing implementation details, and another simulating a user with minimal coding knowledge.

Our study aims to evaluate the aptitude and reliability of AI agents in generating fisheries models, addressing critical questions about their potential to democratize technical expertise while maintaining scientific rigor.

## Methods

### Overview

Our methodological approach centered on developing comprehensive specification sheets for each fisheries modeling task. These detailed instruction sets were designed to guide AI agents through complex analytical workflows while minimizing ambiguity and potential errors.


### AI Agents and Large Language Models: Technical Background

#### Large Language Model Fundamentals

Large language models (LLMs) are advanced neural networks designed to generate human-like responses through next-token prediction. These models are trained on vast internet-sourced datasets, enabling them to perform complex reasoning tasks, including mathematical calculations, logical reasoning, and computer code generation.

#### AI Agent Architecture

An AI agent represents a sophisticated software framework that leverages LLMs through a structured interaction protocol. Unlike simple chatbots, these agents can: - Iterate through multiple computational steps - Make autonomous decisions about analysis workflows - Generate and execute code - Self-correct and refine outputs

#### Model Context Protocol

The Model Context Protocol (MCP) provides a critical framework for guiding LLM interactions. When presented with a task, an AI agent: 1. Receives a comprehensive initial instruction set 2. Analyzes the task's requirements 3. Determines appropriate response strategies (e.g., explanation, code generation, clarification request) 4. Executes specialized tools for specific actions

#### Advantages and Challenges

**Strengths:** - Autonomous workflow generation - Minimal human intervention required - Ability to self-correct and adapt

**Potential Limitations:** - Risk of "context poisoning" (diverging from intended task) - Potential generation of logically flawed solutions - Variability in output consistency

**Mitigation Strategies:** - Developing detailed specification sheets - Implementing rigorous prompting techniques - Conducting multiple replicate runs to assess reliability

Our study aims to systematically evaluate these AI agents' performance in complex scientific modeling tasks, addressing both their transformative potential and inherent challenges.

#### Specification Sheet Development

We created specification sheets that provided exhaustive instructions for each modeling task. These documents included: - Introduction and context - Precise aims of the analysis - Detailed data methodology - Comprehensive analysis methodology - Explicit instructions for the AI agent - Technical context and constraints - Workflow description - Required directory structure - Metadata specifications

The specification sheets drew on prominent scientific references to ensure statistical and methodological accuracy. By providing all instructions upfront, we followed recent recommendations that AI systems perform more reliably with comprehensive initial guidance [@laban2025llms].

#### Experimental Design

We implemented a rigorous experimental design to assess AI agent performance:

1.  **Multiple AI Models**: We tested two large language models—Claude Sonnet Version 4 and Kimi K2—to evaluate inter-model variability.

2.  **Replicate Runs**: We conducted 10 independent runs for each prompt and model. This approach accounts for the inherent randomness in AI-generated outputs and allows us to quantify response consistency. Ten replicates are widely considered sufficient to capture response variability in computational studies [@laban2025llms].

3.  **Comprehensive Evaluation**: Each task was assessed using multivariate rubrics that evaluated both quantitative outputs (e.g., statistical significance) and qualitative performance through categorical outcome metrics.

#### Technical Implementation

We utilized Roo Code agentic AI software (v3.25.1, VS Code extension v1.103.0) in 'Code' mode. The software was configured with auto-approval for tasks including: - File read/write operations - API request retries - Model context protocol interactions - Sub-task creation - Code execution - Self-questioning and todo list management

Manual intervention was required only for terminal commands, which were carefully vetted for safety before approval.

We drew best practice prompting strategies to write these instructions [@brownLLMecology].

We conducted 10 independent replicates for each prompt across both models. This approach accounts for the inherent randomness in large language model (LLM) responses, with ten replicates widely considered sufficient to capture response variability [@laban2025llms].

After each task, Roo Code initiated an 'attempt_completion' protocol to finalize the agent's interactions. We then captured comprehensive metadata, including: - Chat history - Token input and output - Estimated computational cost

We evaluated each task using multivariate rubrics that: - Quantitatively assessed statistical outputs (e.g., p-values) - Qualitatively scored outputs against predefined categorical metrics

For each task, we calculated two key performance indicators: Aptitude and Reliability [@laban2025llms].

Aptitude is a measure of the quality of the Agent's best responses and was defined:

$$
\text{Aptitude} = \frac{1}{k} \sum_{i=1}^{k} S_{(i)}
$$

where $S_{(i)}$ are the top $k$ scores (with $k = \lceil 0.9 \times n \rceil$, $n = 10$ replicates), sorted in descending order.

Reliability is a measure of the consistency of the Agent's responses across the ten replicates and was defined: $$
\text{Reliability} = \frac{1}{k} \sum_{i=1}^{k} \frac{S_{(i)} - S_{(1)}}{S_{(i)}}
$$

where $S_{(1)}$ is the score of the best response, and $S_{(i)}$ are the top $k$ scores (with $k = \lceil 0.9 \times n \rceil$, $n = 10$ replicates), sorted in descending order.  

### Von Bertalanffy Growth Model Test Case

#### Rationale and Significance

The von Bertalanffy (VB) growth model represents a fundamental analytical task in fisheries stock assessment. We selected this model for several critical reasons:

1.  **Disciplinary Specificity**: VB growth modeling is a highly specialized task primarily used in fisheries science, making it an ideal test of AI agents' domain-specific capabilities.

2.  **Complexity Gradient**: While seemingly straightforward, the task involves nuanced statistical challenges, particularly in parameter estimation and confidence interval calculation.

#### Experimental Design

**Data Simulation** We generated a single, carefully controlled dataset with known growth parameters to: - Minimize variability introduced by data heterogeneity - Enable precise assessment of parameter recovery accuracy - Provide a standardized benchmark for AI performance

**Specification Requirements** The AI agents were tasked with: - Estimating VB growth function parameters - Calculating confidence intervals via bootstrapping - Predicting fish size at a specific age (5 years) - Generating confidence intervals for size predictions

**Analytical Objectives** Our primary goals were to evaluate the AI agents' ability to: - Recover known parameters accurately - Implement bootstrapping techniques correctly - Produce statistically sound estimates with appropriate uncertainty quantification

The detailed specification sheet, drawing from established fisheries modeling references (e.g., [@ogle2016introductory]), provided comprehensive guidance for the AI agents.

### Generalized linear model of fish habitat

### Yield per recruit analysis

## Results

### VBF

Alex doing plot 

No point going to mark code for VBF because it basically works for every run. 
Just write up why run 11 and run 13 broke. Explain why it broke. 

VBF: plot of accuracy of parameter estimates, and confidence intervals, for each parameter. 
(bar plot, or just %s? )

### GLM 

Richard doing the plot 

## Discussion

When starting out a fisheries modeller won't have fully informed instructions. These take time to write. We have shown that well worded instructions have an eelemnt of repeatability, in that results are reasonably consistent.


## 4 Conclusion

## Acknowledgements

## Data availability

-   plus link to repository

## Conflict of Interest


## References



## Supplemental material


## Tables

-   All abbreviations must be defined in footnotes. Footnote symbols: †, ‡, §, ¶, should be used (in that order) and \*, \*\*,\*\*\* should be reserved for P-values
-   **TABLE 1.** xxx

## Figure Legends

-   Figures and supporting info should be supplied as separate files
    -   [electronic_artwork_guidelines.pdf](https://media.wiley.com/assets/7323/92/electronic_artwork_guidelines.pdf)
    -   In-text Figures: **CAPTION 1 (A)** xxx
    -   Supporting information Figures: **Table S1** xxx; **Appendix S1**
-   Include definitions of any symbols used and define/explain all abbreviations and units of measurement
