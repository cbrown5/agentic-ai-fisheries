---
title: "Automating ecological and fisheries modelling with agentic AI"
bibliography: references.bib
# csl: oneearth.csl
editor: source
format: docx
    #reference-doc: template.docx
---


## Abstract

Fisheries modelling can take years of specialist training to learn, and the availability of suitably skilled modellers is one of the bottlenecks for delivering science that can inform management. Agentic AI systems automate complex computing programming workflows and they will increasingly be used to accelerate the delivery of fisheries modelling for management. However, questions remain about the quality of results from AI agents, because they can create syntactically accurate but logically flawed code, and they do not response consistently if the same question is asked repeatedly. Here, we tested whether a general-purpose agentic AI (Roo Code) can write computer code and interpret results to complete three common ecological and fisheries modelling workflows: (1) fitting a von-Bertalanffy growth curve to data, (2) writing code interpreting results of a generalized linear model relating fish abundance to habitat, and (3) completing a yield per recruit analysis. We used replicate prompts and a structured evaluation rubric to ask if the AI results were accurate and if responses were consistent. The agent successfully and accurately completed all three tasks in some replicates; however, there was variability in the outcomes across replicated agents. Of the two large language models we tested, Claude Sonnet 4.0 performed accurately and consistently across all three tasks. Our findings show that general purpose agentic AI systems can accurately complete ecological and fisheries modelling, but that further work is needed to ensure consistency of results across different replicates. We further show how to design prompts and workflows to achieve more consistent and accurate results and discuss the importance of ensuring human oversight for credibility of modelling that is used in management.

## Keywords

Agentic AI, ecological modelling, generalized linear models, fisheries modelling, large-language models

## Table of contents

## 1. Introduction

Modern ecological sciences, and particularly fisheries, demand extensive technical expertise and time-consuming computational workflows[@punt2025]. Across ecology, the complexity of modelling can be one impediment to the delivery of timely and accurate advice for policy and decision-making. For example, stock assessment reports often cite time constraints as a barrier to conducting comprehensive analyses, with professionals struggling to complete thorough sensitivity analyses or explore multiple scenarios [@punt2025]. Common mistakes in population modelling can also perpetuate to poor management actions on threatened species [@shoemaker2025; @kendall2019persistent]. Environmental research relies on a broad variety of quantitative methods, which can be challenging to learn and are underrepresented in formal training programs [@touchon2016]. Moreover, quantitative environmental scientists frequently lack formal training in programming, making the creation and maintenance of computational code difficult [@hampton2017]. Creating and sharing reproducible code that is easily understood and is reuseable by others is an even greater challenge [@Culina2020]. Thus, the limited number of trained modelers with expertise in the complexities of ecological systems creates a bottleneck for environmental assessments and exacerbates challenges associated with data availability [@dichmont2025].

Large language models (LLMs) and agentic AI offer a transformative opportunity to automate complex fisheries modelling workflows [@bistarelli2025]. By separating code creation from model design and data acquisition, these technologies can potentially lower the technical barriers for non-experts [@jansen2025leveraging]. For experts, LLMs are particularly valuable in streamlining routine coding tasks, enhancing efficiency, and reducing errors, thereby freeing time for higher-level model design and scientific reasoning [@Chen2023]. For instance, AI-generated code has been applied in biological pest control distribution modelling [@scheepens2024], and in the autonomous synthesis of insect repellents [@bran2024], reducing development time and errors. The ability of LLMs to automate code generation mean that generative AI will become a pervasive modelling tool [@cooper2024], however, questions remain regarding the reliability and quality of AI-generated code [@wills2024] and AI supported science [@johnson2024]. For example, LLMs can create believable but fictitious information [@scheepens2024], or produce code that executes but is logically flawed [@hocky2022; @jansen2025leveraging]. Thus, to effectively use LLM for coding, users need to be familiar enough with the programming language to detect anomalies in AI outputs [@cooper2024]. Evaluations of LLM output against standardized examples therefore plays an important role in determing the boundaries of appropriate use. Evaluations of coding tools are particularly an imperetive in fisheries science where results are often used to inform contentious management decisions. Guidelines for the performance of LLMs are needed to inform fisheries scientists about (A) what tasks can be completed with LLMs and (B) how can LLMs best be used to facilitate accurate fisheries modelling.

Two trends in LLM development are rapidly expanding their applications to scientific coding and modelling. The first is the expanding context window of LLMs. The context window is the amount of information (measured as tokens) that an LLM can process in a single chat thread. The first release of chatGPT had a context window of 4096 tokens, (approximately 3000 words), whereas GPT5 has a window of 400,000 tokens [@gpt-5is]. This increase means LLMs can follow much more complex and detailed sets of instructions to successfully complete a task .

The second trend is increasingly sophisticated AI assistant software (Fig. 1a) and agentic AI software (Fig. 1b) [@ferrag2025]. Unlike simple chatbots that are limited to text responses, assistants and agents can use tools (e.g. via the Model Context Protocol) to run software ranging from basic file handling and running R scripts, to advanced web browsing and remote database access . Combining tools with LLMs allows autonomous feedback loops between code generation, error checking, interpretation and task progression (Fig. 1b). For example, the Roo Code agent considers a user’s initial prompt and generates its own ‘to do list’ [@roocode2025]. It then works through this checklist, solving bugs as it goes until it has completed the to do list. These feedback loops repeatedly improve the accuracy of task completion (Bistarelli et al. 2025; Jansen et al. 2025). The prompts required to initiate an agent and ensure successful task completion are long, detailed and specific sets of instructions (i.e. specification sheets), which differentiates them from the typical 1-2 sentence prompts used with chatbots [@ma2025; @zamfirescu2023]. Agents can write computer programs that solve software bugs, find solution similar to humans and solve machine learning programs to the level of a capable human expert [@rondon; @swe-benc; @chan]. Bespoke agents that are customized to specific biological modeling problems are also showing promise [@jansen2025leveraging; @Spillias2025ecosystemmodel]. What is not clear is if off-the-shelf agent software, which was developed for software programming, can be applied to solve technical and highly specialized research problems.

![](Shared/Figure1.png)

**Figure 1** Workflows for a researcher conducting an analysis using (a) a normal workflow augmented by an AI coding assistant and (b) an analysis automated with an AI agent.

Also add ALex's revised figures

Here, we test whether AI agents can transform specification sheets into reliable ecological and fisheries models by examining three increasingly complex tasks: (1) von Bertalanffy growth curve fitting, (2) generalised linear modeling of fish-habitat relationships, and (3) yield per recruit analysis. Our objective was to inform use of these tools by expert modellers and contribute to improving the standards for how generative AI is used in modelling workflows. Our results also have implications for the design of bespoke modelling agents. Performance of the agents was assessed from the perspective of an expert user who knows the intended outcomes but wants to automate code generation and writing. We conducted a detailed analysis of the AI-generated outputs to better understand the strengths and limitations of the agentic AI. We used the Roo Code agent software and compared two different LLMs. Roo code was developed for autonomous computer programming [@roocode2025], primarily in commercial software development contexts, so its suitability for technical science questions is not guaranteed. Additionally, we tested the agent’s ability to complete tasks with the R program, which is popular with environmental statisticians but less represented in LLM training data than languages such as C. Fisheries science may present a particular challenge for agentic AI because of the limited availability of domain-specific training data.

## 2. Methods

### Overview

We developed comprehensive specification sheets for each fisheries modelling task. These detailed instruction sets were designed to guide agentic AI through complex analytical workflows while minimising ambiguity and potential errors. For each case study, we tested the agent with two LLMs, one that was relatively cheap and open-source and one that is proprietary and relatively expensive. We ran 10 replicates for an autonomous agent for each LLM by case study combination, to allow for non-determinism in LLM responses. Each run of the agent was then evaluated using a standarised rubric that scored the results to quantitative and semi-quantitative criteria, as was appropriate to each case study. We then summarized the scores across the 10 replicates with the accuracy, reliability and aptitude statistics.

### Experimental Design

Our experimental approach was to test two LLMs (Claude Sonnet 4.0 and Kimi K2) against each case study. Each case study by LLM combination was replicated 10 times to account for the inherent randomness in AI-generated outputs [@yuan2025fp32deathchallengessolutions]. Ten replicates is widely considered sufficient to capture response variability in computational studies [@laban2025llms]. Each replicate was run independently in different chat threads and different Visual Studio Code sessions, to avoid replicates sharing information via the cache. Note that Roo Code has no inbuilt memory feature that persists across sessions, so later replicates could not inadvertently learn from the results of earlier replicates [@roocode2025]. We then were able to calculate quality scores for the agent responses across the replicate runs for each of our criteria.

### Evaluation statistics

For quantitative criteria we simply scored each replicate 0/1 on whether it achieved the correct answer or not (with an error tolerance of 5% to allow for some randomness in numerical estimation algorithms). Note that quantitative evaluation statistics, like the root mean squared error, were not useful because the agents tended to either get the correct answer, be off by orders of magnitude, or be unable to provide an answer at all. We scored semi-quantitative criteria on ordinal scales, then normalized them to a range of 0-1 for further analysis.

The scores were then summarized across replicates by their accuracy. Accuracy was defined as the mean of the scores across replicates. Let ( \bar{S}\_j ) be the mean normalized score for question ( j ) (averaged across replicates), and let there be ( m ) questions:

$$
\text{Accuracy} = \frac{1}{m} \sum_{j=1}^{m} \bar{S}_j
$$

The scoring criteria were further aggregated into groups of criteria that were of similar types. Aptitude and reliability were then calculated by these groups [@laban2025llms]. Aptitude measures how well the agent can do when performing near its best and was quantified as the 90th percentile of the criteria-level means:

$$
\text{Aptitude} = \text{Quantile}_{0.9}(\{\bar{S}_j\}_{j=1}^m)
$$

Reliability measures how consistent the agents are in providing the same results and was measured as one minus the difference between the 90th and 10th percentiles of the criteria-level means: $$
\text{Reliability} = 1 - \left( \text{Quantile}_{0.9}(\{\bar{S}_j\}_{j=1}^m) - \text{Quantile}_{0.1}(\{\bar{S}_j\}_{j=1}^m) \right)
$$

where ( \text{Quantile}\_p ) denotes the ( p )-th quantile of the set of question-level means. Note that an agent could have low aptitude but high reliability if it consistently provided inaccurate answers.

We also compared cost (US dollars) to accuracy for two of our case studies that had clear quantitative criteria. Cost is an integrated measure of total tokens used to describe the task by the human (input), prompting and tool-use by the agent (input) and LLM responses (output). Output tokens are about five times more expensive than input tokens. Our hypothesis was that more expensive replicates would be more complete and therefore more accurate.

### Large language models

For LLMs we chose to test Claude Sonnet 4.0 and Kimi K2, the latest versions of these models available at the time of the study. Claude Sonnet 4.0 is well recognised as an industry leader for programming and reasoning tasks [@swe-benc]. Roo Code has been extensively developed with Sonnet in mind, so is likely also optimized for Claude Sonnet 4.0’s quirks. Other popular choices are the openAI models (e.g. GPT 5.0), but our experience is that they perform slightly less consistently for R code than Claude Sonnet 4.0.

We also ran the agent with Kimi K2 as the LLM. Kimi K2 is a much cheaper alternative to Claude Sonnet 4.0, but which has also shown industry leading coding abilities [@swe-benc]. Kimi K2 was built with tool-use in mind, so we tested it here to see if it could obtain similar results to Claude Sonnet 4.0 at a lower cost.

### Agent software

There are a growing number of agent software platforms, were we tested one popular option 'Roo Code' (ranked third for use by apps on https://openrouter.ai/rankings at time of writing 2025-09-24).

We utilized Roo Code agentic AI software (v3.25.1, VS Code extension v1.103.0) in 'Code' mode. The LLMs were accessed via the OpenRouter API. The software was configured with auto-approval for tasks including: - File read/write operations - API request retries - Model context protocol interactions - Sub-task creation - Code execution - Self-questioning and todo list management

Manual intervention was required only for terminal commands, which were carefully vetted for safety before approval. After each task, Roo Code initiated an 'attempt_completion' protocol to finalize the agent's interactions. We then captured comprehensive metadata, including: the chat history, token input and output, estimated computational cost.

### Case studies

We followed similar guidelines for creating specification sheets for each of the three case studies. These specifications provided detailed instructions for each modelling task. These documents included: - Introduction and context - Precise aims of the analysis - Detailed data methodology - Comprehensive analysis methodology - Explicit instructions for the agentic AI - Technical context and constraints - Workflow description - Required directory structure - Metadata specifications. We drew best practice prompting strategies to write these instructions, including using examples and specifying a consistent structure and format for outputs [@brownLLMecology].

The specification sheets drew on fisheries and ecology text books [@haddon; @ogle2018; @zuur2009mixed] to ensure statistical and methodological accuracy. By providing all instructions upfront, we followed recent recommendations that AI systems perform more reliably with a single comprehensive initial prompt [@laban2025llms].

The specifications sheets were developed from scratch based on what our references considered to be best practice for each modelling task. We then ran some pilot tests with the agent to ensure specification sheet completeness. Minor updates were made to the specification sheets to ensure that the output was generated in a standardized way (e.g. more specific instructions for writing parameters into a csv file). The final specification sheets were then kept constant for all replications in the evaluation.

The full specification sheets and marking rubrics are provided in the supplemental material.

#### Von Bertalanffy Growth Model Test Case

Our simplest test for the agents was to fit a von Bertalanffy growth model to some size at age data. The von Bertalanffy was chosen because it is a common in fisheries science but rare in other disciplines and fitting it requires use of non-linear statistical algorithms [@ogle2018]. Thus, this task involves nuanced statistical challenges, particularly in parameter estimation and confidence interval calculation.

We simulated a single size-at-age dataset with known growth parameters based on *Bolbometopon muricatum* (bumphead parrotfish) [@taylor2018parrotfish]. The AI agents were tasked with: Estimating VB growth function parameters, calculating confidence intervals via bootstrapping, predicting fish size at a specific age (5 years) and generating 95% confidence intervals for predictions of size.

The specification sheet had detailed instructions for this task, including instructions for using the `nls()` function to fit the von Bertalanffy, the growth model equation and to use the `boot` package for bootstrapping (supplementary material XXX, [@ogle2018]). We also tested a second simplified specification sheet, which we tested with Claude Sonnet 4.0 because of its near 100% accuracy on all tasks. This second sheet did not include any equations or recommendations for R functions.

We instructed the replicate agents to complete a csv file of parameter values. The agent was given an empty csv file that had only headings, such that it provided results to us in a standardized way. We then scored the responses for accuracy on each parameter and confidence interval. Each replicate and parameter estimate was scored a `1` if it was within 5% of our our estimated value and scored `0` otherwise.

#### Generalized linear model of fish habitat

Our second task was to test the relationship between abundance of a juvenile *B. muricatum* (a large coral reef fish) and cover of two different types of coral. We used data from an existing study that found coral loss was impacting juveniles of a threatened fish species [@hamilton2017]. Note that past studies on this data used different methods, so the LLMs could not have been trained on this exact analysis for this exact data. Further, conversations with the LLMs used here suggested they had no knowledge of the @hamilton2017 study specifically.

The study design included 49 surveys of coral reefs in the Kia region of the Solomon Islands. At each site juveniles were counted by divers on five by 50m long transects. At the same transects benthic cover (percent) of different habitat types was also recorded using the point intersect transect method. For this analysis transects were aggregated to produce one value per site. We summed the fish counts and took averages over the transects for two benthic habitat types: branching coral and soft coral.

We asked the AI agents to answer four questions:

1.  Does fish abundance depend on branching coral cover?
2.  What is the direction and strength of the relationship between fish abundance and branching coral cover?
3.  Does fish abundance depend on soft coral cover?
4.  What is the direction and strength of the relationship between fish abundance and branching coral cover?

The analysis aims, meta-data and approach were all completely described in the specification sheet, including the specific R functions and packages to use. We asked the agent to generate two reports, a report with model verification figures and interpretation, and a second report that had a complete write-up of the methods, results and discussion.

One author (CJB) then manually marked the reports on criteria for: Interpretation. Were statistics correctly interpreted, did the agent come to the correct conclusion? Completeness. Did the agent complete all the tasks we requested? Technical implementation: Did the agent create the figures we requested, and did the scripts it wrote all execute without errors (for some replicates the agent wrote the scripts but did no execute them)

We also included two criteria for 'bonus points'. The first was for whether each agent looked for and identified confounding among the covariates in the dataset. We deliberately left this instruction out of the specification sheets to test whether the agents would follow statistical best practice and look at confounding [@zuur2009mixed]. The second bonus points were for additional insights in the write-up of the results.

There were many replicates where the agents wrote the code but did not execute the code to complete the figures and render the report as a document from the source code (rmarkdown or Rmd format). Therefore, we used this order of priority for awarding marks: We used the written reports as the primary source of information. If an Rmd file was created, but not rendered, we manually rendered it. If the Rmd did not render due to an error we read the the Rmd file. If there was no Rmd file, or the Rmd file was missing key sections, we looked to see if there were saved figures and at the code.

#### Yield per recruit analysis

The yield per recruit (YPR) analysis was our most complex fisheries modelling task, designed to test the agents' ability to integrate multiple biological parameters and population models. We tasked the AI agents with performing a YPR analysis for *Bolbometopon muricatum* (bumphead parrotfish) to evaluate the effects of different size limits on fishery yield [@taylor2018parrotfish; @haddon; @haddon2011].

The agents were required to calculate YPR curves for four management scenarios: a baseline with no size restrictions and three size limit scenarios (400 mm, 500 mm, and 600 mm minimum length). For each scenario, the agents needed to determine three critical fisheries reference points: Fmax (fishing mortality producing maximum yield), Ymax (maximum yield per recruit), and F01 (the more conservative reference point where the yield curve slope equals 10% of the slope at the origin).

The specification sheet provided the biological parameters including von Bertalanffy growth parameters (Linf = 1070 mm, K = 0.15 per year, t0 = -0.074 years), natural mortality (M = 0.169 per year), length-weight relationship coefficients (a = 1.168 × 10\^(-6), b = 3.409), and selectivity parameters (delta = 0.001). Maximum age was set at 29 years with initial recruitment of 1000 individuals.

The agents were instructed to implement a multi-step analytical workflow: (1) model growth using the von Bertalanffy equation, (2) convert length-at-age to weight-at-age using allometric relationships, (3) calculate age at 50% selectivity (A50) for each size limit using the inverse von Bertalanffy equation, (4) model fishing selectivity using logistic functions, (5) apply population dynamics using the Baranov catch equation incorporating both fishing and natural mortality, (6) calculate yield per recruit across a range of fishing mortalities, and (7) estimate reference points from the resulting yield curves.

The specification sheet included all necessary equations formatted as R functions: the von Bertalanffy growth equation, its inverse form for calculating age at length, length-weight conversion, logistic selectivity functions, and the Baranov catch equation for calculating catch given mortality rates [@haddon]. Agents were explicitly instructed to exclude mortality from the first age class and to use steep selectivity curves (small delta values) for size limit scenarios.

Expected deliverables included a completed CSV file containing all calculated reference points and yield curve plots comparing all four management scenarios.

## 3. Results

Overall both LLMs showed high aptitude at completing the range of tasks we set, but consistency varied across the replicates (fig. 2), as we will now discuss for each case study.

![](Shared/Figure-comparison-png.png) **Figure 2** Examples of accurate and erroneous output. (a) The von Bertlanffy case study, where all replicates produced accurate plots of the growth curve. Examples of Claude Sonnet 4.0 producing predictions from the GLM on the appropriate (b) and inappropriate (c) y-axis scale. Examples of (d) Claude Sonnet 4.0 producing accurate yield-per-recruit curves and (e) Kimi K2 producing inaccurate yield-per-recruit curves.

### VBF

The Claude Sonnet 4.0 LLM with the full specification completed the von Bertlanaffy task with 100\% accuracy across all 10 replicates (fig. 3). The Kimi K2 LLM estimated most parameters with more than 80\% accuracy. However, we note that Kimi K2’s output needed some cleaning before we could produce summary results. In particular, the LLM used names for the ‘length at age 5’ parameter that were inconsistent with the convention we proposed in the spec.

The chat logs revealed that in two replicates Kimi K2 made simple syntax errors in the R code, resulting in a script that did not complete the task error free. The agent however failed to realise it had not completed the task fully (a confabulation) and proceeded to manually enter parameter values into incorrect positions in the parameter table.

The Claude Sonnet 4.0 LLM also performed close to 100\% accuracy for all parameters when using the simpler specification that had no instructions about algorithms to use for fitting the von Bertalanffy (fig 3a). It scored 100\% accuracy for parameters K and Linf and the estimate of the mean length at age 5. Accuracy was lower for the confidence intervals for length at age 5. However, when reviewing the R code it created we noticed it commonly used the t distribution for estimating confidence intervals, compared to our solution that used bootstrapping. Hence, the confidence intervals were greater than 5\% different to ours in 30-40\% of cases, but still estimated with a valid algorithm. Likewise, the values for t0 were often \>5\% but \<10\% different to our values.

![](Shared/Outputs/fig2-vbf_Claude-Kimi-simple.png)

**Figure 3** Results for the von Bertalanffy case study. (a) Mean accuracy for the two LLMs and each parameter estimate. (b) Summary of mean accuracy, aptitude and reliability for the two LLMs parameters.

### GLM

The Claude Sonnet 4.0 LLM was able to consistently complete the full task with high accuracy (\>86\%), aptitude (100\%) and reliability (\>71\%) including from data import, data wrangling, statistical analysis, verification of analysis and report writing (fig. 4). It always conducted the model simplification using likelihood ratios (as requested) and used the appropriate p-value (100\% accuracy) to interpret the results in the written reports. Inconsistencies for the Claude Sonnet 4.0 LLM were that it did not always produce diagnostic plots that were relevant for count data and it did not always provide interpretation of the diagnostic plots. Strangely, despite its high accuracy on all other tasks, the Claude Sonnet 4.0 LLM was almost incapable of following the simple instruction to use a log scale on the y-axis of predictive plots (40\% accuracy).

The Kimi K2 LLM did not perform reliably or accurately on the GLM task (fig 4). It frequently failed to complete reports (no report created 30\% of the time) or did not follow instructions precisely. It often created code for the likelihood ratio test, but then used a different or made-up p-value in reporting (fig. 4a). In some cases, the LLM generated code for plots that executed successfully when we tested it, but did not complete the full reporting task. We found cases of fabrication in the chat logs for the Kimi K2 agent. The Kimi K2 agent often stated it had completed the reports, although it had not done so. The only criteria where the Kimi K2 LLM outperformed the Claude Sonnet 4.0 LLM was on using the correct scale for the predictive plots (80\% vs 40\%, fig. 4a).

The Kimi K2 LLM had high aptitude for completeness (92\%) and technical implementation (96\%) (fig 4b), indicating that it could perform well at these types of tasks, just not reliably.

Neither the Kimi K2 (<5\%) nor the Claude Sonnet 4.0 LLM (<25\%) performed well on the ‘bonus points’ tasks: additional important considerations that we did not explicitly request in the GLM spec. Claude Sonnet 4.0 occasionally made further interpretation of the data that we judged as valuable contributions. Neither model ever picked up on confounding between the two covariates and neither model suggested that confounding as something that should be tested for.

![](Shared/Outputs/figure-3.png)

**Figure 4** Results for the GLM case study. (a) Mean accuracy for the two LLMs and each criteria from the rubric. (b) Summary of mean accuracy, aptitude and reliability for the two LLMs by criteria types.

### YPR

The Claude Sonnet 4.0 LLM demonstrated moderate performance (50-100\% accuracy) on the yield per recruit (YPR) analysis (fig. 5). Claude Sonnet 4.0 consistently completed the assigned tasks (90 - 100\%), generating executable code, producing plots, and populating parameter tables as specified. In contrast, the Kimi K2 LLM was less reliable, frequently failing to complete the workflow (20\% were complete). Kimi K2 often produced code that contained errors or was not executed, resulting in incomplete outputs.

Claude Sonnet 4.0 generally produced accurate results for most parameters. However, a recurring issue was the incorrect application of fishing and natural mortality rates, particularly the misallocation of mortality to the first age class despite explicit instructions to exclude it. This error substantially affected the estimation of maximum yield (Ymax), leading to lower accuracy for this parameter. Kimi K2’s performance was more variable, with occasional high aptitude scores but frequent failures to complete the analysis or generate valid outputs.

Overall, Claude Sonnet 4.0 provided robust and reproducible results for most aspects of the YPR analysis, but both LLMs exhibited limitations in handling nuanced methodological details.

![](Shared/Outputs/figure-4-YPR-quant_and_qual.png)

**Figure 5** Results for the yield-per-recruit case study. (a) Mean accuracy for the two LLMs for criteria related to technical implementation of the code, (b) mean accuracy for criteria related to the quantitative results.

### Cost and accuracy

We compared the average accuracy scores against total cost for the quantitative evaluations for the VBF and YPR case studies (fig. 6). While Kimi K2’s accuracy was lower (mostly \<25\%) compared to Claude Sonnet 4.0 (\>2\5%), it was also cheaper to run (\<\$0.1-0.4) than Claude Sonnet 4.0 (\<\$0.2-0.8). However, within each model, higher-cost solutions did not consistently correspond to higher accuracy. For example, Kimi K2 twice failed to complete the VBF task, becoming stuck in an endless tool-calling loop, wasted tokens and never found a solution (fig. 6a). Similarly, Claude Sonnet 4.0’s most expensive solutions for the YPR tended to be less accurate.

![](Shared/Outputs/figure-5-cost-accuracy.png)

**Figure 6** Cost versus accuracy for (a) the von Bertalanffy growth model and (b) the yield per recruit case studies.

## 4. Discussion

This study shows that agentic AI systems can accurately and consistently automate common fisheries modelling workflows. We also showed that performance varies across current state-of-the-art LLMs. Our evaluation of agentic AI across three increasingly complex fisheries modelling tasks - von Bertalanffy growth model fitting, generalized linear modelling, and yield per recruit analysis—reveals that Claude Sonnet 4.0 consistently achieved high accuracy (near 100\%) for simple tasks and moderate performance (~50\%) for complex tasks, while Kimi K2 showed variable and often unreliable performance. Our results align with evaluations of LLMs from other areas of ecology that also find high performance when prompts and agent software are calibrated to specific tasks. For example, automated literature synthesis [@gougherty2024], extracting biodiversity data from unstructured text [@castro2024] and ecological model generation (Spillias et al. 2025) A key point of difference between our study and earlier studies that looked at model and code development is that we used generic agent software, rather than software that was calibrated to a specific science application (e.g. @Spillias2025ecosystemmodel; @jansen2025leveraging). Our results are important because they pave the way for immediate integration of LLMs and agents into workflows for ecological and fisheries modelling.

The performance patterns revealed distinct capabilities and limitations across different types of modeling tasks and LLMs. It is therefore likely that LLM choice will remain an important consideration in the near-term. Claude Sonnet 4.0 showed consistently high performance across the complexity gradient from simple parameter estimation to complex population modeling, maintaining reliable task completion and accurate statistical interpretation. In contrast, Kimi K2 exhibited substantial variability, with particular weaknesses in complex analytical workflows where it frequently failed to complete tasks, made confabulations  by incorrectly assuming task completion,   or generated plausible but incorrect outputs—a form of hallucination particularly concerning in fisheries science where undetected errors can result in poor stock management.  Similar variability has been observed in evaluations of LLMs on logic and reasoning tasks (e.g. @laban2025llms; @zhu2024large). More generally, open-source models like Kimi K2 currently lack the complexity of proprietary LLMs like  Claude Sonnet 4.0 and therefore often perform more poorly on logic and reasoning tasks (e.g. @laban2025llms; @zhu2024large).

These findings align with broader trends in AI-assisted scientific computing while revealing domain-specific challenges for ecological and fisheries modelling applications [@bistarelli2025; @scheepens2024]. Unlike general programming tasks where LLMs benefit from extensive training data, fisheries modelling represents a specialised domain with limited publicly available code and methodological examples. This scarcity likely contributed to the variable performance we observed and some of the quirks of the model output, particularly for complex tasks requiring integration of multiple biological parameters and statistical methods. For example, in the GLM case –study, Claude Sonnet 4.0 inconsistently provided predictions on a log-transformed scale and in the yield-per-recruit model, it applied natural mortality to new recruits, contrary to our request. These minor inconsistencies occurred within complete analyses, highlighting the importance of human oversight and review of code. Further, none of the agents looked for confounding among variables in the GLM case study, despite that being part of best-practice statistical workflows [@zuur2009mixed]. We suggest agent software can assist experts by   accelerating coding and model development; however, it cannot replace the expert knowledge needed to create precise and detailed specification sheets and to review outputs to identify and correct errors.  

Several important limitations must be considered when interpreting these results and their applicability to real-world fisheries applications. First, our evaluation was limited to three modelling tasks using simulated or controlled datasets, which may not capture the complexity and messiness of real-world fisheries data. Future work should assess agent performance on larger workflows, such as a full stock assessment that requires fitting multiple statistical models to estimate life-history parameters (e.g. growth, mortality, selectivity) before fitting a biomass dynamics model [@hilborn2013]. Similarly,  ecological applications of agents would benefit from studies using complex multivariate datasets (e.g. ref) and statistical problems requiring multiple databases, as automated data wrangling remains elusive [@jansen2025leveraging]. Although our workflows were simplified, - the high performance observed here suggests that many components of real-world workflows can be partly automated. Second,  we tested only one agent platform (Roo Code) and two LLMs, so results may not generalize to other AI systems or configurations. Specifically, Roo Code is optimised for the Claude Sonnet 4.0 series of models, whereas Kimi K2 is a newer model that may perform better with different agent software. Nevertheless, the Kimi K2 LLM may still provide useful starting points for more efficient modelling tasks for experts who can refine the code.  Finally, LLMs and their integration into agents are evolving rapidly, meaning our performance results may quickly become outdated. These limitations do not undermine our core findings because we established a framework for evaluating  agent performance under controlled conditions; allowing systematic comparisons and providing a foundation for future evaluations with the next generation of LLMs or with other modelling tasks.

Our study provides practical lessons for the integration of agents into modelling workflows. These lessons could be applied when using general purpose agents like Roo Code, as well as for developing bespoke ecological modelling agents [@Spillias2025ecosystemmodel]. The primary step in our workflow is structuring the agent’s response with detailed specification sheets that cover analysis aims, methodology, software preferences, meta-data and instructions for outputs. Our approach therefore extends previous evaluations of LLM performance in biological science, which have tested quality of responses to one-off prompts [@scheepens2024] or simple agents that iterate to correct syntax errors [@jansen2025leveraging]. The creation of specification sheets takes careful thought from modelling experts, and while generative AI could be used to assist in this creation, expert oversight is essential because any logical errors in  specifications  will likely flow through to the agent and generate misleading results. For example, an agent designed for fitting ecological models to data defaulted to the most common fitting algorithm, rather than using the algorithms that were most appropriate to the data [@Spillias2025ecosystemmodel]. 

Our study raises broader issues of transparency and trust that are critical to address if AI-assisted modelling is to be adopted for advancing scientific knowledge and applications of models to management. Scientists using generative AI are required to be transparent about its use and take full responsibility for mistakes [@cooper2024]. Therefore, we do not recommend the use of  AI-generated code without review of the code by experts (colloquially called ‘vibe-coding’). Agents should be viewed as tools that accelerate expert modeller’s workflows , rather than tools for novices to generate management advice. While LLMs can be useful aids for novices to learn quantitative methods, they cannot replace human training in math, statistical reasoning and computer coding [@campbell2024; @cooper2024]. Further, scientists must retain understanding of their research [@messeri2024artificial; @johnson2024], since this is fundamental to providing informed advice to stakeholders. Retaining expert knowledge is especially critical in fisheries science, where methods (including stock models) need to be transparent and defensible in public and policy contexts. Moreover, fisheries modelling extends beyond code execution, it involves the human dimension of ecological reasoning, stakeholder engagement, and ethical decision making. A final key issue with LLM and agent use is data privacy, which would require a modification of our approach. Our approach uses cloud-hosted LLMs, but  the data privacy policies of those providers are variable. Fisheries modelling often uses sensitive data, and data-use agreements may prohibit uploading to cloud services or leaving the country of origin. Likewise, socio-ecological studies may involve human interview data with strict confidentiality requirements. In fact, the most common question raised by fisheries scientists when we have run training workshops on LLMs is “how can I keep the data private?”. One potential solution is the use of open-source models like Kimi K2, which can be downloaded and hosted on local servers or computers, giving researchers greater control over data privacy. Although open-source models perform less accurately on evaluations than  proprietary models [e.g. @laban2025llms], they may still give researchers a valuable head start on modelling tasks.


## 5. Conclusion

We found that agentic AI built with contemporary LLMs and generic agent software can accurately complete common fisheries and ecological modelling tasks, but issues remain with obtaining consistently accurate results. These systems represent a promising tool for use in fisheries science, offering the potential to accelerate the delivery of scientific advice and address bottlenecks in the capacity of quantitatively trained scientists. Our findings provide a basis for the following recommendations:

1. The ecological and fisheries modelling communities need to develop transparent protocols for sharing AI-aided workflows, including specification sheets for specific tasks. These can then be re-used and potentially assist novice users to complete modelling tasks more accurately and efficiently. 

2.	A future research priority is to test agents on real-world datasets with missing data and methodological complexities, and expand evaluations to more complex modeling problems. For example, agents could be applied to full stock assessment process that incorporate multiple empirical models (growth curves, selectivity curves etc.) and population dynamics modelling to inform management reference points, or the automation of models for management strategy evaluation.

Overall, we recommend that current agentic AI systems are best used by expert fisheries modellers who have have the knowledge to critically evaluate and correct results.

## Acknowledgements

CJB was supported by a Future Fellowship (FT210100792) from the Australian Research Council. We'd like to thank for S. Spillias for his insights on an earlier version. 

## Data availability

Code is available at: https://github.com/cbrown5/agentic-ai-fisheries

Data including complete agent transcripts and results of the evaluations are available at: https://github.com/cbrown5/agentic-ai-results

## Generative AI use in Research Statement 

In addition to the AI tools described in the manuscript, we used Github Copilot with Claude Sonnet 4.0 to assist in generating code for the figures. Copilot (GPT 4-Turbo) was asked to 'peer-review' a complete draft of the manuscript and those reviews were used to identify improvements. The authors take full responsibility for all code and content in this work. 

## Conflict of Interest

None to declare

## Supplemental material

## References


