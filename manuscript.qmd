---
title: "Automating fisheries modelling with agentic AI"
bibliography: references.bib
# csl: oneearth.csl
editor: source
format: docx
    #reference-doc: template.docx
---

Journal: Fish and Fisheries

## Abstract

-   250 words

-   6 keywords - alphabetical order

\[re-write this as its for expert users as well. \] Fisheries modelling can take years of specialist training to learn. Agentic AI systems automate complex computing programming workflows and could lower the technical barrier for fisheries modelling. However, questions remain about the quality of AI derived models. Here we test whether agentic AI can write computer code to complete three common types of fisheries models. We test an agentic AI system (Roo Code) on its ability to complete three fisheries modelling workflows, from data analysis to report write-up: (1) von-Bertalanffy parameter estimation, (2) generalized linear modelling of fish-habitat relationships, (3) yield per recruit analysis. We use replicate prompts and a rubric to evaluate the AI generated reports. We find that... We show how careful prompting of the AI system can deliver high quality reports for fisheries modelling problems. Our results show that agentic AI systems can already complete complex fisheries workflows, particularly if users provide context-rich prompts. We discuss implications for fisheries science including equity of access to technical expertise and the immediate priority of increasing transparency of AI use in fisheries science, potential pitfalls, and the need for a community of practice around prompting.

## Table of contents

## 1. Introduction

Modern ecological sciences, and particularly fisheries, demand extensive technical expertise and time-consuming computational workflows[@punt2025]. The complexity of modelling can be one impediment to the delivery of timely and well-informed advice to fishery managers [@cotter2004]. For example, stock assessment reports often cite time constraints as a barrier to conducting comprehensive analyses, with professionals struggling to complete thorough sensitivity analyses or explore multiple scenarios [@punt2025]. Fisheries science and ecology more generally relies on a broad variety of quantitative methods, which can be challenging to learn and are underrepresented in formal training programs [@touchon2016]. Moreover, quantitative environmental scientists frequently lack formal training in programming, making the creation and maintenance of computational code difficult [@hampton2017]. Creating and sharing reproducible code that is easily understood and is useable by others is an even greater challenge [@Culina2020]. Thus, the limited number of trained modellers with expertise in the complexities of fisheries science creates a bottleneck for stock assessments and exacerbates challenges associated with data availability [@dichmont2025].

\[Leigha can we just broaden above para a bit so it feels relevatn to all ecologists, not just fisheries (but still with fisheries as a prime example)\]

Large language models (LLMs) and AI agents offer a transformative opportunity to automate complex fisheries modelling workflows [@bistarlli2025]. By separating code creation from model design and data acquisition, these technologies can potentially lower the technical barriers for non-experts [@jansen2025leveraging]. For experts, LLMs are particularly valuable in streamlining routine coding tasks, enhancing efficiency, and reducing errors, thereby freeing time for higher-level model design and scientific reasoning [@Chen2023]. For instance, AI-generated code has been applied in biological pest control distribution modelling [@scheepens2024], and in the autonomous synthesis of insect repellents [@bran2024], reducing development time and errors. However, questions remain regarding the reliability and quality of this AI-generated code [@wills2024]. For example, LLMs can produce fabricated information [@scheepens2024], or produce code that executes but fails to follow best scientific practice for a particular type of computational task [@hocky2022]. Thus, to effectively use LLM for coding, users need to be familiar enough with the programming language to detect anomalies in AI outputs [@cooper2024].

Previous tests of LLMs for statistical analysis have revealed significant challenges for the application of generative AI to statistics [@ellis2023]. While LLMs can generate executable code, a recent study found less than 50% of generated scripts were technically accurate [@jansen2025leveraging]. Notably, code execution rates improved when AI models could self-correct [@zhu2024large]. Key insights have emerged about effective LLM-human interaction: contextual information is crucial for making informed statistical choices; providing reference code and statistical approaches guides AI responses [@zhu2024large]; and one-shot prompting with comprehensive examples often yields more reliable results than multi-turn conversations [@laben2025llms]. Further, agents that self correct mistakes have much higher accuracy than single response chat bots [@jansen2025leveraging]. Users must be skillful in writing effective prompts, which are the instructions or inputs given to the LLM, and evaluating the responses [@lubiana2023]. Prompts are being replaced by  detailed specification sheets of long, precise instruction sets written in plain English [@chen2025]. These specifications can articulate analysis requirements clearly, making them accessible to both expert and novice users.

A growing body of research is exploring the use of LLMs as assistants or chat bots (e.g. ChatGPT or GitHub Copilot) that generate code in response to human prompts [e.g. @Bistarelli]. The human then runs this code, collects and interprets the outputs, and writes the report (Figure 1A). In contrast, recent developments have enabled the creation of AI agents, where an LLM is embedded in a closed-loop system that can autonomously generate, execute and refine computer code, actively manage computation steps, and evaluate results to complete a specified taskâ€”with minimal human intervention (Figure 1B) [@ferrag2025].

\[Can we merge above paras? Doesn't quite work as is. Not sure how to fix though. We have two ideas to cover: quality of prompting, using agents with spec sheets\]

Here we test whether AI agents can transform specification sheets into reliable fisheries models by examining three increasingly complex tasks: (1) von Bertalanffy growth curve fitting, (2) generalized linear modeling of fish-habitat relationships, and (3) yield per recruit analysis. We conduct detailed analysis of the generated responses to better understand the strengths and limitations of the AI. Evaluations of agent-LLM pairs in other fields suggest we should be successful. However, fisheries science may present a special challenge to agents, because of the relatively limited amount of training data available to LLMs relative to topics such as game programming. We are also testing the agents in the R program, which is popular with environmental statisticians, but for which there is much more limited training code available than common languages like C.

## 2. Methods

### Overview

We developed comprehensive specification sheets for each fisheries modelling task. These detailed instruction sets were designed to guide AI agents through complex analytical workflows while minimizing ambiguity and potential errors. TODO write overview summary.

![](Shared/Figure1.png)

**Figure 1** Workflows for a researcher conducting an analysis using (a) a normal workflow augmented by an AI coding assistant and (b) an analysis automated with an AI agent.

### AI Agents and Large Language Models: Technical Background

Large language models (LLMs) are complex neural networks trained on large bodies of text and images and able to generate novel text. They predict what comes next when given text as input, also called prompting. The unit of prediction is a token, each of which is a sequence of letters that is common in the training set. The LLMs most people are familiar with are calibrated to be assistants, such that they respond seamlessly to questions.

LLM's have a finite 'memory' for text they receive such that they can create coherent sentences, phrases and whole documents that are coherent with complex prompts \[Citation?\]. This memory, called the context window, is orders of magnitude larger than the short queries that are the most common prompts on commercial web platforms. The best coding models at writing have context windows of 200,000 tokens, equivalent to several PhD theses worth of text, larger models have context windows of 1M tokens plus. LLM training data often includes reasoning and computer code, so LLMs can also exhibit rational reasoning and the ability to implement that reasoning in various programming languages. The reasoning ability and large context windows give them the ability to read substantial documents, reason about complex scientific questions and then create analyses to test ideas. Some LLMs are also multi-modal and can take images as inputs, so can accurately perform tasks such as interpretation of scientific figures.

Generative AI agents are software systems that can operate autonomously to complete a task. Unlike simple chatbots, these agents can: - Iterate through multiple computational steps - Make autonomous decisions about analysis workflows - Generate and execute code - Self-correct and refine outputs. Agents embed one or more LLMs within a software system that includes feedback loops among LLM inputs, LLM outputs and other software tools (Figure 1). These feedback loops replace some parts of workflows traditionally performed by humans. For instance, a simple agent workflow could look like: (1) a human requests an analysis is complete; (2) the agent passes the query to an LLM; (3) the LLM responds with code; (4) the agent passes the code to the R program and the code is run; (5) the agent passes the results of the R script back to the LLM as the next input... And so on, until the task is stopped manually or by trigger points built into the agent.

Agent software is made possible by a framework for LLMs to communicate to software tools: the Model Context Protocol. This is a set of rules that tells an LLM what tools are available to it, what rules apply to those tools (such as file type restrictions) and how to structure its response if it wants a tool to be used. When the agent software parses each LLM response it is looking for tool calls, which it then passes into the tool.

A tool can be any software locally on the user's computer or hosted remotely on the cloud, so long as that software can be interacted via code (an application program interface). Simple agents have tools for reading files, writing to files and running terminal commands. This enables them to read a file's content, then updated it such as adding new code. With terminal commands they can do tasks such as search directories, create files, delete files and run applications such as R scripts. More sophisticated tools include the ability to connect to and navigate in web browsers, conduct web searches and access remotely hosted databases.

The advent of agents is leading to longer and more complex prompts, which have been called 'specifications' \[cite AI talk on youtube\]. These specifications describe a task with detailed instructions. Our aim was to develop specifications for common fisheries modelling tasks and then test a leading agent software for its ability to complete those tasks.

#### Experimental Design

Our experimental approach was to test two LLMs (Claude Sonnet 4.0 and Kimi K2) against each case-study. Each case-study by LLM combination was replicated 10 times to account for the inherent randomness in AI-generated outputs. Ten replicates are widely considered sufficient to capture response variability in computational studies [@laban2025llms]. We quarantined each replicate in a different VScode session to avoid replicates sharing information via the cache. We then were able to calculate quality scores for the agent responses across the replicate runs for each of our criteria.

For LLMs we chose to test Claude Sonnet 4.0 and Kimi K2, the latest versions of these models available at the time of the study. Claude Sonnet 4.0 is well recognized as an industry leader for programming and reasoning tasks. Roo Code has been extensively developed with Sonnet in mind, so is likely also optimized for Sonnet's quirks. Other popular choices are the openAI models (e.g. GPT 5.0), but we have found perform slightly less consistently for R code than Sonnet.

Kimi K2 is a much cheaper alternative to Sonnet, but which has also shown industry leading coding abilities. Kimi K2 was built with tool-use in mind, so we tested it here to see if it can obtain similar results to Sonnet at a lower cost.

### Case-studies

We followed similar guidelines for creating specification sheets for each of the three case-studies. These specifications provided detailed instructions for each modelling task. These documents included: - Introduction and context - Precise aims of the analysis - Detailed data methodology - Comprehensive analysis methodology - Explicit instructions for the AI agent - Technical context and constraints - Workflow description - Required directory structure - Metadata specifications. We drew best practice prompting strategies to write these instructions, including using examples and specifying a consistent structure and format for outputs [@brownLLMecology].

The specification sheets drew on fisheries and ecology text books [@haddon; @ogle2018; @zuur2009mixed] to ensure statistical and methodological accuracy. By providing all instructions upfront, we followed recent recommendations that AI systems perform more reliably with comprehensive initial guidance [@laban2025llms].

UP TO HERE Explain a bit about how we fine-tuned these.

The full spec sheets are provided in the supplemental material because they are too long to provide here.

#### Von Bertalanffy Growth Model Test Case

Our simplest test for the agents was to fit a von Bertalanffy growth model to some size at age data. The von Bertalanffy was chosen because it is a common in fisheries science but rare in other disciplines and fitting it requires use of non-linear statistical algorithms. Thus, this task involves nuanced statistical challenges, particularly in parameter estimation and confidence interval calculation.

We simulated a single size at age dataset with known growth parameters based on *Bolbometopon muricatum* [@taylor2018parrotfish]. The AI agents were tasked with: Estimating VB growth function parameters, calculating confidence intervals via bootstrapping, predicting fish size at a specific age (5 years) and generating 95% confidence intervals for size predictions.

The specification sheet had detailed instructions for this task, including instructions for using the `nls()` function to fit the von Bertalanffy, the growth model equation and to use the `boot` package for bootstrapping (supplementary material XXX, [@ogle2018]).

We instructed the replicate agents to complete a csv file of parameter values. The agent was given an empty csv file that had only headings, such that it provided results to us in a standardized way. We then scored the responses for accuracy on each parameter and confidence interval. Each replicate and parameter estimate was scored a `1` if it was within 5% of our our estimated value.

### Generalized linear model of fish habitat

Our second task was to test the relationship between abundance of a juvenile *Bolbometopon muricatum* (a large coral reef fish) and cover of two different types of coral. We used data from an existing study that found coral loss was impacting juveniles of a threatened fish species [@hamilton2017]. The study design included 49 surveys of coral reefs in the Kia region of the Solomon Islands. At each site juvenile were counted by divers on five by 50m long transects. At the same transects benthic cover (percent) of different habitat types was also recorded using the point intersect transect method. For this analysis transects were aggregated to produce one value per site. We summed the fish counts. We took averages over the transects for two benthic habitat types: branching coral and soft coral.

We asked the AI agents to answer four questions:

1.  Does fish abundance depend on branching coral cover?
2.  What is the direction and strength of the relationship between fish abundance and branching coral cover?
3.  Does fish abundance depend on soft coral cover?
4.  What is the direction and strength of the relationship between fish abundance and branching coral cover?

UP TO HERE

Designed to test the limits, included some 'easy asks' as well as more difficult challenges (confounded variables).

Order of priority for awarding marks: Used report as primary source of information. If this wasn't knitted we knitted it. If it didn't knit we read the rmd file. If there was no rmd file, or the rmd file was missing key sections, we looked for figures and at the code. Did this for p-values, prediction plots, diagnostic plots.

### Yield per recruit analysis

Used parameter for *Bolbometopon muricatum* [@taylor2018parrotfish]. Based code on @haddon

### Agent set-up

There are a large number of LLMs available that can be mixed and matched with a growing number of agent software platforms. Here we test the Roo Code agent software. At the time of writing Roo Code was the number two ranked software by its user base. We picked this software because it allows greater customisation than the number one ranked software (Cline), so could be more useful for bespoke scientific applications.

We utilized Roo Code agentic AI software (v3.25.1, VS Code extension v1.103.0) in 'Code' mode. The LLMs were accessed via the OpenRouter API. The software was configured with auto-approval for tasks including: - File read/write operations - API request retries - Model context protocol interactions - Sub-task creation - Code execution - Self-questioning and todo list management

Manual intervention was required only for terminal commands, which were carefully vetted for safety before approval. After each task, Roo Code initiated an 'attempt_completion' protocol to finalize the agent's interactions. We then captured comprehensive metadata, including: - Chat history - Token input and output - Estimated computational cost

### Evaluating the results

Evaluated from two perspectives

An expert user with advanced coding skills can employ highly specific, technically rich prompts that reference particular statistical models, coding frameworks, and data structures, instructing the AI to optimise parameters, debug scripts, or automate simulations (Jansen et al. 2025). An expert would evaluate on criteria such as statistically robustness, accuracy and precision, reproducibility, efficiency, technical detail, scientific rigor, reliability, technical implementation, and quality and completeness of outputs. In contrast, a novice user with limited coding experience would frame prompts in plain-English expecting the AI to recommend analytical pathways and translate technical requirements into accessible workflows. Their evaluation would assess whether the results appear consistent with their expectations, if steps clearly explained and terminology is easy to understand, whether the AI improves accessibility, clarity and reduces technical barriers, and outputs are actionable for decision-making.Â 

We evaluated each task using multivariate rubrics that: - Quantitatively assessed statistical outputs (e.g., p-values) - Qualitatively scored outputs against predefined categorical metrics

For each task, we calculated three key performance indicators: Accuracy, Aptitude, and Consistency [@laban2025llms].

First, we computed the mean score, normalized by each criteria's maximum value, for each question across all replicates. Then, for each model and user type, we calculated: Accuracy is defined as the mean of the question-level means. Aptitude is the 90th percentile of the question-level means. Consistency is one minus the difference between the 90th and 10th percentiles of the question-level means.

Let ( \bar{S}\_j ) be the mean normalized score for question ( j ) (averaged across replicates), and let there be ( m ) questions:

$$
\text{Accuracy} = \frac{1}{m} \sum_{j=1}^{m} \bar{S}_j
$$

$$
\text{Aptitude} = \text{Quantile}_{0.9}(\{\bar{S}_j\}_{j=1}^m)
$$

$$
\text{Consistency} = 1 - \left( \text{Quantile}_{0.9}(\{\bar{S}_j\}_{j=1}^m) - \text{Quantile}_{0.1}(\{\bar{S}_j\}_{j=1}^m) \right)
$$

where ( \text{Quantile}\_p ) denotes the ( p )-th quantile of the set of question-level means.

## 3. Results

### VBF

The Sonnet LLM with the full spec completed the von Bertlanaffy task with perfect accuracy across all 10 replicates (fig. 2). The Kimi LLM estimated most parameters more than 80% accurate. However, we note that Kimi's output needed some cleaning before we could produce summary results. In particular, the LLM used names for the 'length at age 5' parameter that were inconsistent with the convention we proposed in the spec.

The chat logs revealed that in two replicates Kimi made simple syntax errors in the R code, resulting in a script that did not complete the task error free. It thus manually entered parameter values into the parameter table, putting those values in the wrong positions.

The Sonnet LLM also performed close to 100% accuracy for all parameters when using the simpler specification that had no instructions about algorithms to use for fitting the von Bertalanffy (fig 2a). It scored 100% accuracy for parameters K and Linf and the estimate of the mean length at age 5. Accuracy was lower for the confidence intervals for length at age 5. However, when reviewing the R code it created we noticed it commonly used the t distribution for estimating confidence intervals, compared to our solution that used bootstrapping. Hence the confidence intervals were greater than 5% different to ours in 30-40% of cases, but still estimated with a valid algorithm. Likewise, the values for t0 were often \>5% but \<10% different to our values.

![](Shared/Outputs/fig2-vbf_Claude-Kimi-simple.png)

**Figure 2** Results for the von Bertalanffy case-study. (a) Mean accuracy for the two LLMs and each parameter estimate. (b) Summary of mean accuracy, aptitude and reliability for the two LLMs parameters.

### GLM

The Sonnet LLM was able to consistently complete the full task, from data import, data wrangling, statistical analysis, verification of analysis and report writing (fig. 3). It always conducted the model simplification using likelihood ratios (as requested) and used the appropriate p-value to interpret the results in the written reports. Inconsistencies for the Sonnet LLM were that it did not always produce diagnostic plots that were relevant for count data and it did not always provide interpretation of the diagnostic plots. Strangely, despite its high accuracy on all other tasks, the Sonnet LLM was almost incapable of following the simple instruction to use a log scale on the y-axis of predictive plots.

The Kimi LLM did not perform reliably or accurately on the GLM task (fig 3). It frequently failed to complete reports (no report created 30% of the time) or did not follow instructions precisely. It often created code for the likelihood ratio test, but then used a different or made-up p-value in reporting (fig 3a). In some cases the LLM generated code for plots that executed successfully when we tested it, but did not complete the full reporting task. We found cases of fabrication in the chat logs for the Kimi agent. The Kimi agent often stated it had completed the reports, although it had not done so. The only criteria where the Kimi LLM outperformed the Sonnet LLM was on using the correct scale for the predictive plots (fig 3a).

The Kimi LLM had high aptitude for completeness (92%) and technical implementation (96%) (fig 3b), indicating that it could perform well at these types of tasks, just not reliably.

Neither the Kimi or the Sonnet LLM performed well on the 'bonus points' tasks: additional important considerations that we did not explicitly request in the GLM spec. Sonnet occasionally made further interpretation of the data that we judged as valuable contributions. Neither model ever picked up on confounding between the two covariates and neither model suggested that confounding as something that should be tested for.

![](Shared/Outputs/figure-3.png)

**Figure 3** Results for the GLM case-study. (a) Mean accuracy for the two LLMs and each criteria from the rubric. (b) Summary of mean accuracy, aptitude and reliability for the two LLMs by criteria types.

### YPR

The Sonnet LLM demonstrated moderate performance on the yield per recruit (YPR) analysis. Sonnet consistently completed the assigned tasks, generating executable code, producing plots, and populating parameter tables as specified. In contrast, the Kimi LLM was less reliable, frequently failing to complete the workflow. Kimi often produced code that contained errors or was not executed, resulting in incomplete outputs.

Sonnet generally produced accurate results for most parameters. However, a recurring issue was the incorrect application of fishing and natural mortality rates, particularly the misallocation of mortality to the first age class despite explicit instructions to exclude it. This error substantially affected the estimation of maximum yield (Ymax), leading to lower accuracy for this parameter. Kimi's performance was more variable, with occasional high aptitude scores but frequent failures to complete the analysis or generate valid outputs.

Overall, Sonnet provided robust and reproducible results for most aspects of the YPR analysis, but both LLMs exhibited limitations in handling nuanced methodological details, underscoring the need for expert oversight and careful prompt engineering.

![](Shared/Outputs/figure-4-YPR-quant_and_qual.png)

**Figure 4** Results for the yield-per-recruit case-study. (a) Mean accuracy for the two LLMs for criteria related to technical implementation of the code, (b) mean accuracy for criteria related to the quantitative results.

### Cost and accuracy

Show grouped results here on cost and overal accuracy across all case-studies? Facet by case-study to plot: cost (x-axis) accuracy (y-axis) LLM (colour).

## 4. Discussion

The analysis of Sonnet and Kimi on the VBGF, GLM and YPR models provides valuable insight into the strengths and limitation of these two LLMs in the context of fisheries management. Their performance varies when linguistically tasked with managing a diverse range of fisheries models that vary in complexities and difficulty.

When starting out a fisheries modeller won't have fully informed instructions. These take time to write. We have shown that well worded instructions have an element of repeatability, in that results are reasonably consistent. Additionally, providing detailed prompts does not guarantee accurate outcomes, as performance largely depends on the inherent characteristics of each model. Sonnet model consistently outperform Kimi regardless of prompt specificity. Importantly, all models carry intrinsic error that fisheries modellers must recognise to avoid introducing bias into decison-making processes. Hallucinations by models are errors that fisheries modellers must pay particular attention to. For instance, Kimi exhibited this characteritics in it analysis. Hallucination typically occurs when the model lacks sufficient evidence in its training data to support its response [@chen2025]. This limitation provides insight into the strengths and capabilities of both models when applied to complex linguistic task in fisheries analysis. It emphasise the need for continuous improvement and vigorous evaluation of AI agents for reliable and efficient LLM for fisheries.  

Discuss issue with Kimi making up values. can't be trusted.

Vibe coding, not for high stakes situations. Ok to try somethign out or for learning. If using agents you should check and verify all code. Often need to iterate a lot on the prompts to develop a good prompt. Also creating multiple versions and implementations of a project to compare and get ideas.

Overall we found Sonnet to be a reliable and accurate implementer of the statistical task and that it could consistently follow instructions. However, it could not be relied upon as an expert statistician - it did not consistently provide best-practice verification plots and it never suggested we check for confounding. Therefore, it is a powerful tool if used by an expert, or given clear steps on the workflow required for a statistical valid analysis.

Kimi was less reliable if left on its own. its tendency to generate plausible yet incorrect outputs undermines its reliability, posing a risk in fisheries applications where high accuracy is essential for decision making. Basiscally would be good as an AI aid, but not trusted to work on its own. Its work needs more correction and checking. Note that could be interation with Roo Code set-up which is optimised for Sonnet.

## 5. Conclusion

## Acknowledgements

## Data availability

-   plus link to repository

## Conflict of Interest

## References

## Supplemental material

## Tables

-   All abbreviations must be defined in footnotes. Footnote symbols: â€ , â€¡, Â§, Â¶, should be used (in that order) and \*, \*\*,\*\*\* should be reserved for P-values
-   **TABLE 1.** xxx

## Figure Legends

-   Figures and supporting info should be supplied as separate files
    -   [electronic_artwork_guidelines.pdf](https://media.wiley.com/assets/7323/92/electronic_artwork_guidelines.pdf)
    -   In-text Figures: **CAPTION 1 (A)** xxx
    -   Supporting information Figures: **Table S1** xxx; **Appendix S1**
-   Include definitions of any symbols used and define/explain all abbreviations and units of measurement